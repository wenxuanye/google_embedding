{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5nKcowWfmwY"
      },
      "source": [
        "## I share the know-how to speed up training.\n",
        "\n",
        "## Archtecture\n",
        "<!-- This is a overview of model.  \n",
        "![model](https://github.com/motono0223/kaggle_public/blob/main/2022_guie/GUIE_Model.png?raw=true)   -->\n",
        "\n",
        "Training the entire model takes a long time because the backbone model is very large.  \n",
        "In this notebook, I propose to split model into a backbone model and a projection model like following steps.\n",
        "\n",
        "## Steps\n",
        "- Step1: infers the backbone model to get training data.  \n",
        "<!-- ![backbonemobel](https://github.com/motono0223/kaggle_public/blob/main/2022_guie/GUIE_BackboneModel.png?raw=true) -->\n",
        "\n",
        "- Step2: trains the projection model with output of step1.  \n",
        "<!-- ![projectionmodel](https://github.com/motono0223/kaggle_public/blob/main/2022_guie/GUIE_ProjectionModel.png?raw=true) -->\n",
        "\n",
        "- Step3: save weights of entire \"model\".  \n",
        "In this notebook, the entire \"model\" shares the same weights with backbone model and projection model.\n",
        "\n",
        "<!-- ## Note\n",
        "If you want to use the data augment method for training model, please consider to add the process of training \"model\" above as finetuning. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train in Colan enviroment \n"
      ],
      "metadata": {
        "id": "UpNeCZR3y4Df"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i72153AaDJds",
        "outputId": "ac352de3-9a25-4318-a007-0be70fb4e4e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (5.0.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.18.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "# !pip install git+https://github.com/innat/transformers -U -q > /dev/null\n",
        "import os\n",
        "def is_colab_env():\n",
        "    is_colab = False\n",
        "    for k in os.environ.keys():\n",
        "        if \"COLAB\" in k:\n",
        "            is_colab = True\n",
        "            break\n",
        "    return is_colab\n",
        "    \n",
        "if is_colab_env():\n",
        "    !pip install transformers\n",
        "    !pip install tensorflow_addons\n",
        "from transformers import CLIPProcessor, TFCLIPVisionModel, CLIPFeatureExtractor\n",
        "\n",
        "# from kaggle_datasets import KaggleDatasets\n",
        "\n",
        "import re\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import normalize\n",
        "import pickle\n",
        "import json\n",
        "import tensorflow_hub as tfhub\n",
        "from datetime import datetime\n",
        "import gc\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "from mpl_toolkits import axes_grid1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make sure to use the TPU of Colab\n"
      ],
      "metadata": {
        "id": "1KRxrCfQzGyv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khrTPhLcR39a",
        "outputId": "3163c37a-82da-4aad-caaf-0aa92a3b0faf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on TPU  grpc://10.116.84.18:8470\n",
            "REPLICAS:  8\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    # set: this is always the case on Kaggle.\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set the contant config value"
      ],
      "metadata": {
        "id": "CTsbyF_0zNu8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCwQB_L1NGoH",
        "outputId": "802e0a58-ccbe-43c5-e4c8-945d7ece1adc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "clip-vit-large-patch14\n"
          ]
        }
      ],
      "source": [
        "class config:\n",
        "    VERSION = 60\n",
        "\n",
        "    SEED = 42\n",
        "    RESUME = False\n",
        "    RESUME_EPOCH = 0\n",
        "    RESUME_WEIGHT = \"\"\n",
        "\n",
        "    model_type = \"clip-vit-large-patch14\"\n",
        "    EFF_SIZE = 0\n",
        "    EFF2_TYPE = \"\"\n",
        "    # 336/224\n",
        "    IMAGE_SIZE = 336\n",
        "    BATCH_SIZE_INFER = 16 * strategy.num_replicas_in_sync\n",
        "    BATCH_SIZE_TRAIN = 100 * strategy.num_replicas_in_sync\n",
        "    N_CLASSES = 9691\n",
        "    EMB_DIM = 64\n",
        "    EPOCHS = 300\n",
        "    LR = 0.0012\n",
        "\n",
        "    save_dir = \"./\"\n",
        "\n",
        "    TRAIN = True\n",
        "    \n",
        "    DEBUG = False\n",
        "    \n",
        "    TTA = 1\n",
        "\n",
        "\n",
        "# Function to seed everything\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "seed_everything(config.SEED)\n",
        "\n",
        "MODEL_NAME = None\n",
        "if config.model_type == 'effnetv1':\n",
        "    MODEL_NAME = f'effnetv1_b{config.EFF_SIZE}'\n",
        "elif config.model_type == 'effnetv2':\n",
        "    MODEL_NAME = f'effnetv2_{config.EFF2_TYPE}'\n",
        "elif \"swin\" in config.model_type:\n",
        "    MODEL_NAME = config.model_type\n",
        "elif \"conv\" in config.model_type:\n",
        "    MODEL_NAME = config.model_type\n",
        "else:\n",
        "    MODEL_NAME = config.model_type\n",
        "\n",
        "config.MODEL_NAME = MODEL_NAME\n",
        "print(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The root path of our datasets in Google cloud"
      ],
      "metadata": {
        "id": "cDERhBqAzS1w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yu0W1G3ddQz3"
      },
      "outputs": [],
      "source": [
        "dict_target_dataset = {\n",
        "    # base\n",
        "        # \"guie-imagenet1k-mini1-tfrecords-label-0-999\" : \"gs://wenxuanye/imagenet1k\",\n",
        "        \"guie-products10k-tfrecords-label-1000-10690\" : \"gs://wenxuanye/products10k\",\n",
        "        \"guie-glr2021mini-tfrecords-label-10691-17690\" : \"gs://wenxuanye/google_landmark\",\n",
        "        \"omnibench-label-17691\":\"gs://wenxuanye/omnibenchmark\",\n",
        "        # \"deepfashion\":\"gs://wenxuanye/deepfashion\"\n",
        "\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "PREPROC_DATASET_DIR = f\"./preproc/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZuGi10XOuiW"
      },
      "source": [
        "# Read the tfrecord datasets for classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAmYgnXmFE3P"
      },
      "outputs": [],
      "source": [
        "def deserialization_fn(serialized_example):\n",
        "    parsed_example = tf.io.parse_single_example(\n",
        "        serialized_example,\n",
        "        features={\n",
        "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
        "            'image/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
        "        }\n",
        "    )\n",
        "    image = tf.image.decode_jpeg(parsed_example['image/encoded'], channels=3)\n",
        "    image = tf.image.resize(image, size=(config.IMAGE_SIZE, config.IMAGE_SIZE))\n",
        "    label = tf.cast(parsed_example['image/class/label'], tf.int64)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "def arcface_format(image, label_group):\n",
        "    return {'inp1': image, 'inp2': label_group}, label_group\n",
        "\n",
        "def rescale_image(image, label_group):\n",
        "    image = tf.cast(image, tf.float32) * 255.0\n",
        "    return image, label_group\n",
        "\n",
        "# Data augmentation function\n",
        "def data_augment(image, label_group):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    #image = tf.image.random_flip_up_down(image)\n",
        "    image = tf.image.random_hue(image, 0.01)\n",
        "    image = tf.image.random_saturation(image, 0.70, 1.30)\n",
        "    image = tf.image.random_contrast(image, 0.80, 1.20)\n",
        "    image = tf.image.random_brightness(image, 0.10)\n",
        "\n",
        "    return image, label_group\n",
        "\n",
        "# Dataset to obtain backbone's inference\n",
        "# output : ( image, label ), (label) \n",
        "def get_backbone_inference_dataset(tfrecord_paths, cache=False, repeat=False, shuffle=False, augment=False):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(tfrecord_paths)\n",
        "    dataset = dataset.shuffle(len(tfrecord_paths)) if shuffle else dataset\n",
        "    dataset = dataset.flat_map(tf.data.TFRecordDataset)\n",
        "    dataset = dataset.map(deserialization_fn, num_parallel_calls=AUTO) # image[0-1], label[0-999]\n",
        "\n",
        "    if augment:\n",
        "        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)  # (image, label_group) --> (image, label_group)\n",
        "    dataset = dataset.map(rescale_image, num_parallel_calls = AUTO)  # image[0-1], label[0-n_classes] --> image[0-255], label[0-n_classes]\n",
        "    dataset = dataset.map(arcface_format, num_parallel_calls=AUTO)   # (image, label_group) --> ({\"inp1\":image, \"inp2\":label_group}, label_group )\n",
        "    if repeat:\n",
        "        dataset = dataset.repeat()\n",
        "    dataset = dataset.batch(config.BATCH_SIZE_INFER)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOPX4LshNXsM"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG13FRxnfmwd"
      },
      "source": [
        "## Distance Margin Layer\n",
        "This notebook uses distance margin layer instead of the ArcFace Layer.  \n",
        "  \n",
        "With ArcFace, the embedding vectors are distributed over the surface of an N-dimensional sphere.   \n",
        "This is because ArcFaceLayer outputs the inner product of its own weight matrix and embedding vector to the next Softmax layer.   \n",
        "Although this embedding vector is easy to work with, it is not a very efficient use of the embedding vector space.  \n",
        "\n",
        "So I introduce a distance margin layer.   \n",
        "This outputs the inverse squared Euclidean distance in the next Softmax layer, not the inner product.  \n",
        "![DistanceMarginLayer](https://github.com/motono0223/kaggle_public/blob/main/2022_guie/GUIE_DistanceLayer.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuwssBrY7qaU"
      },
      "outputs": [],
      "source": [
        "class DistanceMarginLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_classes, s=30, m=0.10, easy_margin=False,\n",
        "                 ls_eps=0.0, sgm=1.0, **kwargs):\n",
        "\n",
        "        super(DistanceMarginLayer, self).__init__(**kwargs)\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.ls_eps = ls_eps\n",
        "        self.easy_margin = easy_margin\n",
        "        self.sgm = sgm\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'n_classes': self.n_classes,\n",
        "            's': self.s,\n",
        "            'm': self.m,\n",
        "            'ls_eps': self.ls_eps,\n",
        "            'easy_margin': self.easy_margin,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(DistanceMarginLayer, self).build(input_shape[0])\n",
        "\n",
        "        self.W = self.add_weight(\n",
        "            name='W',\n",
        "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
        "            initializer='glorot_uniform',\n",
        "            dtype='float32',\n",
        "            trainable=True,\n",
        "            regularizer=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X, y = inputs\n",
        "        y = tf.cast(y, dtype=tf.int32)\n",
        "        \n",
        "        W2 = self.W \n",
        "\n",
        "        X2 = tf.tile(tf.expand_dims(X, 2), [1, 1, self.W.shape[1] ])  # X.shape=[Batch, EmbDim], X2.shape=[Batch, EmbDim, NumClass]\n",
        "\n",
        "        # distance on the N-Dimentional coordinate\n",
        "        dx = tf.math.sqrt( tf.reduce_sum( tf.math.pow( X2 - W2, 2 ), axis=1) ) # dx.shape=[Batch, NumClass]\n",
        "\n",
        "        dx = tf.clip_by_value( dx    , 0.00001, 50.0)\n",
        "        output1 = self.s / tf.math.pow( dx, 2 )\n",
        "        output2 = self.s / tf.math.pow( dx, 2 ) + self.m\n",
        "\n",
        "        one_hot = tf.cast(\n",
        "            tf.one_hot(y, depth=self.n_classes),\n",
        "            dtype=dx.dtype\n",
        "        )\n",
        "\n",
        "        output = (one_hot * output1) + ((1.0 - one_hot) * output2)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Arcmarginproduct class keras layer\n",
        "class ArcMarginProduct(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Implements large margin arc distance.\n",
        "\n",
        "    Reference:\n",
        "        https://arxiv.org/pdf/1801.07698.pdf\n",
        "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
        "            blob/master/src/modeling/metric_learning.py\n",
        "    '''\n",
        "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
        "                 ls_eps=0.0, **kwargs):\n",
        "\n",
        "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.ls_eps = ls_eps\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = tf.math.cos(m)\n",
        "        self.sin_m = tf.math.sin(m)\n",
        "        self.th = tf.math.cos(math.pi - m)\n",
        "        self.mm = tf.math.sin(math.pi - m) * m\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'n_classes': self.n_classes,\n",
        "            's': self.s,\n",
        "            'm': self.m,\n",
        "            'ls_eps': self.ls_eps,\n",
        "            'easy_margin': self.easy_margin,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ArcMarginProduct, self).build(input_shape[0])\n",
        "\n",
        "        self.W = self.add_weight(\n",
        "            name='W',\n",
        "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
        "            initializer='glorot_uniform',\n",
        "            dtype='float32',\n",
        "            trainable=True,\n",
        "            regularizer=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X, y = inputs\n",
        "        y = tf.cast(y, dtype=tf.int32)\n",
        "        cosine = tf.matmul(\n",
        "            tf.math.l2_normalize(X, axis=1),\n",
        "            tf.math.l2_normalize(self.W, axis=0)\n",
        "        )\n",
        "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = tf.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        one_hot = tf.cast(\n",
        "            tf.one_hot(y, depth=self.n_classes),\n",
        "            dtype=cosine.dtype\n",
        "        )\n",
        "        if self.ls_eps > 0:\n",
        "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
        "\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        return output\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX76WJYoMgey"
      },
      "outputs": [],
      "source": [
        "def get_scale_layer(rescale_mode = \"tf\"):\n",
        "    # For keras_cv_attention_models module:\n",
        "    # ref: https://github.com/leondgarse/keras_cv_attention_models/blob/main/keras_cv_attention_models/imagenet/data.py\n",
        "    # ref function : init_mean_std_by_rescale_mode()\n",
        "\n",
        "    # For effV2 (21k classes) : https://github.com/leondgarse/keras_efficientnet_v2\n",
        "\n",
        "    if isinstance(rescale_mode, (list, tuple)):  # Specific mean and std\n",
        "        mean, std = rescale_mode\n",
        "    elif rescale_mode == \"torch\":\n",
        "        mean = np.array([0.485, 0.456, 0.406]) * 255.0\n",
        "        std = np.array([0.229, 0.224, 0.225]) * 255.0\n",
        "    elif rescale_mode == \"tf\":  # [0, 255] -> [-1, 1]\n",
        "        mean, std = 127.5, 127.5\n",
        "    elif rescale_mode == \"tf128\":  # [0, 255] -> [-1, 1]\n",
        "        mean, std = 128.0, 128.0\n",
        "    elif rescale_mode == \"raw01\":\n",
        "        mean, std = 0, 255.0  # [0, 255] -> [0, 1]\n",
        "    else:\n",
        "        mean, std = 0, 1  # raw inputs [0, 255]        \n",
        "    scaling_layer = keras.layers.Lambda(lambda x: ( tf.cast(x, tf.float32) - mean) / std )\n",
        "    \n",
        "    return scaling_layer\n",
        "\n",
        "def get_clip_model():\n",
        "    inp = tf.keras.layers.Input(shape = [3, config.IMAGE_SIZE, config.IMAGE_SIZE])\n",
        "#    backbone = TFCLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "# openai/clip-vit-large-patch14-336\n",
        "# openai/clip-vit-large-patch14\n",
        "    backbone = TFCLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14-336\")\n",
        "    output = backbone({'pixel_values':inp}).pooler_output\n",
        "    return tf.keras.Model(inputs=[inp], outputs=[output])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7aFHEiKv5FS"
      },
      "outputs": [],
      "source": [
        "if config.N_CLASSES%64!=0:\n",
        "  config.N_CLASSES = config.N_CLASSES + 64 - config.N_CLASSES%64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uASDSuOFHQ1G"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_embedding_model_new(frozen=True):\n",
        "    #------------------\n",
        "    # Definition of placeholders\n",
        "    inp = tf.keras.layers.Input(shape = [None, None, 3], name = 'inp1')\n",
        "    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
        "\n",
        "    # Definition of layers\n",
        "    layer_resize = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, [config.IMAGE_SIZE, config.IMAGE_SIZE]), name='resize')\n",
        "    layer_scaling = get_scale_layer(rescale_mode = \"torch\")\n",
        "    layer_permute = tf.keras.layers.Permute((3,1,2))\n",
        "    layer_backbone = get_clip_model()\n",
        "    if frozen:\n",
        "        layer_backbone.trainable = False\n",
        "    # layer_pool = tf.keras.layers.GlobalAveragePooling1D(1024)\n",
        "    # add 1d batch norm\n",
        "    layer_batch_norm = tf.keras.layers.BatchNormalization()\n",
        "    # add prreLU\n",
        "    layer_prelu = tf.keras.layers.PReLU()\n",
        "    layer_dropout = tf.keras.layers.Dropout(0.1)\n",
        "    layer_dense_before_arcface = tf.keras.layers.Dense(1024)\n",
        "    layer_margin = DistanceMarginLayer(\n",
        "        n_classes = config.N_CLASSES, \n",
        "        s = 30, \n",
        "        m = 0.3,  # no margin in this notebook\n",
        "        name=f'head/distancemargin', \n",
        "        dtype='float32'\n",
        "        )\n",
        "    # layer_margin = ArcMarginProduct(\n",
        "    #     n_classes = config.N_CLASSES, \n",
        "    #     s = 30, \n",
        "    #     m = 0.3, \n",
        "    #     name=f'head/arcface', \n",
        "    #     dtype='float32'\n",
        "    #     )\n",
        "\n",
        "    # layer_margin = CurricularAdaCos(out_dim=config.N_CLASSES,\n",
        "    #                                 margin=0.4)\n",
        "    layer_softmax = tf.keras.layers.Softmax(dtype='float32')\n",
        "    layer_adaptive_pooling = tfa.layers.AdaptiveAveragePooling1D(64)\n",
        "    layer_norm = tf.keras.layers.Lambda(lambda x: x, name='embedding_norm')\n",
        "    #------------------\n",
        "    # Definition of entire model \n",
        "    image = layer_scaling(inp)\n",
        "    image = layer_resize(image)\n",
        "    image = layer_permute(image)\n",
        "    backbone_output = layer_backbone(image)\n",
        "    # embed = layer_pool(backbone_output)\n",
        "    embed = layer_dropout(backbone_output)\n",
        "    embed = layer_batch_norm(embed)\n",
        "    embed = layer_prelu(embed)\n",
        "    \n",
        "    embed = layer_dense_before_arcface(embed)\n",
        "    x = layer_margin([embed, label])\n",
        "    output = layer_softmax(x)\n",
        "    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output]) # whole architecture\n",
        "\n",
        "    #------------------\n",
        "    # Definition of embedding model (for submission)\n",
        "    x = layer_adaptive_pooling(x)\n",
        "    emboutput = layer_norm(x)\n",
        "    embed_model = tf.keras.models.Model(inputs = [inp, label], outputs = [emboutput]) # whole architecture\n",
        "    \n",
        "    #------------------\n",
        "    # Definition of backbone model (to obtain training data)\n",
        "    backbone_model = tf.keras.models.Model(inputs = [inp, label], outputs = [backbone_output, label])  \n",
        "\n",
        "    #------------------\n",
        "    # Definition of projection Model (to train projection layers)\n",
        "    # print(backbone_output.shape)\n",
        "    inp_proj = tf.keras.layers.Input(shape = [backbone_output.shape[-1]], name = 'inp_proj')\n",
        "    x = layer_dropout(inp_proj)\n",
        "    # x = layer_pool(inp_proj)\n",
        "    x = layer_batch_norm(x)\n",
        "    x = layer_prelu(x)\n",
        "    x = layer_dense_before_arcface(x)\n",
        "    x = layer_margin([x, label])\n",
        "    output = layer_softmax(x)\n",
        "    projection_model = tf.keras.models.Model(inputs = [inp_proj, label], outputs = [output])\n",
        "\n",
        "    # Note: These 4 models share the same weights each other.\n",
        "    return model, embed_model, backbone_model, projection_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5nGM8XncMglt",
        "outputId": "3f80150b-ab9b-4a9e-cc5a-4dc61a29d112"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at openai/clip-vit-large-patch14-336 were not used when initializing TFCLIPVisionModel: ['clip/text_model/encoder/layers_._11/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._1/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._11/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._6/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._5/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._5/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._4/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._7/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._10/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._5/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._1/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._4/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._10/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._8/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._0/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._10/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._6/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._0/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._7/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._0/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._6/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._4/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._7/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._8/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._7/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._10/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._0/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._1/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._4/self_attn/v_proj/bias:0', 'clip/text_model/final_layer_norm/beta:0', 'clip/text_model/encoder/layers_._5/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._6/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._3/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._5/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._5/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._10/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._7/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._2/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._3/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._11/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._2/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._1/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._1/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._4/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._7/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._2/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._9/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._0/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._1/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._5/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._9/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._11/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._6/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._9/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._5/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._8/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._5/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._1/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._1/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._11/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._6/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._6/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._7/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._1/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._3/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._6/self_attn/q_proj/kernel:0', 'clip/text_projection/kernel:0', 'clip/text_model/encoder/layers_._9/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._2/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._3/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._10/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._1/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._4/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._4/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._2/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._2/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._5/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._0/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._8/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._7/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._5/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._3/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._10/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._9/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._8/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._11/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._3/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._3/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._2/layer_norm2/gamma:0', 'clip/text_model/final_layer_norm/gamma:0', 'clip/text_model/encoder/layers_._1/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._11/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._6/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._3/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._8/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._0/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._8/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._7/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._11/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._7/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._2/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._7/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._7/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._9/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._11/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._9/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._2/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._10/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._5/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._6/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._2/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._3/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._6/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._0/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._8/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._0/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._3/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._10/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._0/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._2/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._0/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._11/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._7/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._10/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._11/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._11/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._4/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._8/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._11/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._11/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._3/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._9/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._5/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._6/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._2/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._10/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._10/layer_norm2/beta:0', 'clip/logit_scale:0', 'clip/text_model/encoder/layers_._4/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._4/self_attn/out_proj/bias:0', 'clip/text_model/embeddings/position_embedding/embeddings:0', 'clip/text_model/encoder/layers_._9/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._8/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._3/self_attn/k_proj/bias:0', 'clip/visual_projection/kernel:0', 'clip/text_model/encoder/layers_._4/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._8/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._0/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._1/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._6/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._8/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._6/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._7/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._3/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._0/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._5/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._8/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._9/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._9/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._9/self_attn/v_proj/bias:0', 'clip/text_model/embeddings/token_embedding/weight:0', 'clip/text_model/encoder/layers_._11/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._3/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._1/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._2/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._2/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._0/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._3/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._2/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._10/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._10/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._10/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._1/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._9/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._4/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._6/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._4/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._4/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._1/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._7/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._4/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._9/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._9/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._8/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._2/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._6/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._8/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._0/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._5/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._11/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._1/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._9/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._0/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._5/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._10/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._4/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._3/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._8/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._7/self_attn/q_proj/kernel:0']\n",
            "- This IS expected if you are initializing TFCLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFCLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFCLIPVisionModel were initialized from the model checkpoint at openai/clip-vit-large-patch14-336.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPVisionModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "    model, emb_model, backbone_model, projection_model = get_embedding_model_new()\n",
        "\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate = config.LR)\n",
        "    projection_model.compile(\n",
        "        optimizer = opt,\n",
        "        loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
        "        metrics = [tf.keras.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n",
        "        )\n",
        "\n",
        "    if config.RESUME:\n",
        "        print(f\"load {config.RESUME_WEIGHT}\")\n",
        "        model.load_weights( config.RESUME_WEIGHT )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qSWkPesHMgpO",
        "outputId": "a4b7d597-3cf8-4e70-e6a5-2c35b21c7d99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inp1 (InputLayer)              [(None, None, None,  0           []                               \n",
            "                                 3)]                                                              \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, None, None,   0           ['inp1[0][0]']                   \n",
            "                                3)                                                                \n",
            "                                                                                                  \n",
            " resize (Lambda)                (None, 336, 336, 3)  0           ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " permute (Permute)              (None, 3, 336, 336)  0           ['resize[0][0]']                 \n",
            "                                                                                                  \n",
            " model (Functional)             (None, 1024)         303507456   ['permute[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)           (None, 1024)         0           ['model[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 1024)        4096        ['dropout_24[0][0]']             \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " p_re_lu (PReLU)                (None, 1024)         1024        ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1024)         1049600     ['p_re_lu[0][0]']                \n",
            "                                                                                                  \n",
            " inp2 (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " head/distancemargin (DistanceM  (None, 9728)        9961472     ['dense[0][0]',                  \n",
            " arginLayer)                                                      'inp2[0][0]']                   \n",
            "                                                                                                  \n",
            " softmax (Softmax)              (None, 9728)         0           ['head/distancemargin[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 314,523,648\n",
            "Trainable params: 11,014,144\n",
            "Non-trainable params: 303,509,504\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0a5LW0tnMgsX",
        "outputId": "4043ae8b-bbba-4c02-fde0-a5b3db2711c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inp1 (InputLayer)              [(None, None, None,  0           []                               \n",
            "                                 3)]                                                              \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, None, None,   0           ['inp1[0][0]']                   \n",
            "                                3)                                                                \n",
            "                                                                                                  \n",
            " resize (Lambda)                (None, 336, 336, 3)  0           ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " permute (Permute)              (None, 3, 336, 336)  0           ['resize[0][0]']                 \n",
            "                                                                                                  \n",
            " model (Functional)             (None, 1024)         303507456   ['permute[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)           (None, 1024)         0           ['model[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 1024)        4096        ['dropout_24[0][0]']             \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " p_re_lu (PReLU)                (None, 1024)         1024        ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1024)         1049600     ['p_re_lu[0][0]']                \n",
            "                                                                                                  \n",
            " inp2 (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " head/distancemargin (DistanceM  (None, 9728)        9961472     ['dense[0][0]',                  \n",
            " arginLayer)                                                      'inp2[0][0]']                   \n",
            "                                                                                                  \n",
            " adaptive_average_pooling1d (Ad  (None, 64)          0           ['head/distancemargin[0][0]']    \n",
            " aptiveAveragePooling1D)                                                                          \n",
            "                                                                                                  \n",
            " embedding_norm (Lambda)        (None, 64)           0           ['adaptive_average_pooling1d[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 314,523,648\n",
            "Trainable params: 11,014,144\n",
            "Non-trainable params: 303,509,504\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "emb_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_etfk-hrDwGn",
        "outputId": "74122ac8-92f6-4174-8e6d-2f95a69fbefe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inp1 (InputLayer)              [(None, None, None,  0           []                               \n",
            "                                 3)]                                                              \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, None, None,   0           ['inp1[0][0]']                   \n",
            "                                3)                                                                \n",
            "                                                                                                  \n",
            " resize (Lambda)                (None, 336, 336, 3)  0           ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " permute (Permute)              (None, 3, 336, 336)  0           ['resize[0][0]']                 \n",
            "                                                                                                  \n",
            " model (Functional)             (None, 1024)         303507456   ['permute[0][0]']                \n",
            "                                                                                                  \n",
            " inp2 (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 303,507,456\n",
            "Trainable params: 0\n",
            "Non-trainable params: 303,507,456\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "backbone_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l3iDqpUHIn5H",
        "outputId": "0c4408fe-23ba-4bd3-b66d-9c7445c849c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inp_proj (InputLayer)          [(None, 1024)]       0           []                               \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)           (None, 1024)         0           ['inp_proj[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 1024)        4096        ['dropout_24[1][0]']             \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " p_re_lu (PReLU)                (None, 1024)         1024        ['batch_normalization[1][0]']    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1024)         1049600     ['p_re_lu[1][0]']                \n",
            "                                                                                                  \n",
            " inp2 (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " head/distancemargin (DistanceM  (None, 9728)        9961472     ['dense[1][0]',                  \n",
            " arginLayer)                                                      'inp2[0][0]']                   \n",
            "                                                                                                  \n",
            " softmax (Softmax)              (None, 9728)         0           ['head/distancemargin[1][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11,016,192\n",
            "Trainable params: 11,014,144\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "projection_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4YZEydFrI4m"
      },
      "source": [
        "# Step1) Inference of backbone model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V5UPgaI-ry2M"
      },
      "outputs": [],
      "source": [
        "def get_num_of_image(file):\n",
        "    return int(file.split(\"/\")[-1].split(\".\")[0].split(\"-\")[-1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ECod4QZErIF_",
        "outputId": "afe5be39-9b45-4d8d-d0ee-bfe8a36a2f0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "// ---------------------------------------\n",
            "\"guie-products10k-tfrecords-label-1000-10690\" : \"gs://wenxuanye/products10k\",\n",
            "['gs://wenxuanye/products10k/guie-products10k-train-00-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-01-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-02-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-03-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-04-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-05-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-06-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-07-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-08-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-09-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-10-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-11-7096.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-12-7096.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-13-7096.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-14-7096.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-15-7096.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-16-7096.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-17-7096.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-18-7096.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-19-7096.tfrec']\n",
            "guie-products10k-tfrecords-label-1000-10690 , number of tfrecords =  20 data length =  141931\n",
            "<PrefetchDataset element_spec=({'inp1': TensorSpec(shape=(None, 336, 336, 3), dtype=tf.float32, name=None), 'inp2': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
            "1109/1109 [==============================] - 592s 527ms/step\n",
            "// ---------------------------------------\n",
            "\"guie-glr2021mini-tfrecords-label-10691-17690\" : \"gs://wenxuanye/google_landmark\",\n",
            "['gs://wenxuanye/google_landmark/guie-glr2021-train-00-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-01-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-02-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-03-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-04-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-05-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-06-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-07-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-08-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-09-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-10-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-11-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-12-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-13-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-14-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-15-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-16-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-17-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-18-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-19-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-20-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-21-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-22-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-23-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-24-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-25-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-26-10908.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-27-10908.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-28-10908.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-29-10908.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-30-10908.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-31-10908.tfrec']\n",
            "guie-glr2021mini-tfrecords-label-10691-17690 , number of tfrecords =  32 data length =  349082\n",
            "<PrefetchDataset element_spec=({'inp1': TensorSpec(shape=(None, 336, 336, 3), dtype=tf.float32, name=None), 'inp2': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
            "2728/2728 [==============================] - 1383s 507ms/step\n",
            "// ---------------------------------------\n",
            "\"omnibench-label-17691\" : \"gs://wenxuanye/omnibenchmark\",\n",
            "['gs://wenxuanye/omnibenchmark/guie-omniv1-train-00-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-01-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-02-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-03-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-04-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-05-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-06-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-07-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-08-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-09-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-10-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-11-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-12-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-13-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-14-19267.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-15-19267.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-16-19267.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-17-19267.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-18-19267.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-19-19267.tfrec']\n",
            "omnibench-label-17691 , number of tfrecords =  20 data length =  385354\n",
            "<PrefetchDataset element_spec=({'inp1': TensorSpec(shape=(None, 336, 336, 3), dtype=tf.float32, name=None), 'inp2': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
            "3011/3011 [==============================] - 1524s 506ms/step\n",
            "total_label = 22416\n",
            "!!! re-define model !!!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at openai/clip-vit-large-patch14-336 were not used when initializing TFCLIPVisionModel: ['clip/text_model/encoder/layers_._11/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._1/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._11/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._6/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._5/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._5/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._4/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._7/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._10/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._5/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._1/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._4/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._10/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._8/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._0/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._10/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._6/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._0/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._7/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._0/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._6/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._4/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._7/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._8/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._7/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._10/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._0/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._1/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._4/self_attn/v_proj/bias:0', 'clip/text_model/final_layer_norm/beta:0', 'clip/text_model/encoder/layers_._5/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._6/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._3/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._5/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._5/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._10/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._7/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._2/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._3/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._11/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._2/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._1/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._1/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._4/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._7/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._2/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._9/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._0/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._1/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._5/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._9/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._11/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._6/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._9/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._5/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._8/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._5/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._1/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._1/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._11/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._6/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._6/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._7/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._1/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._3/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._6/self_attn/q_proj/kernel:0', 'clip/text_projection/kernel:0', 'clip/text_model/encoder/layers_._9/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._2/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._3/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._10/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._1/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._4/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._4/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._2/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._2/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._5/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._0/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._8/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._7/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._5/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._3/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._10/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._9/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._8/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._11/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._3/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._3/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._2/layer_norm2/gamma:0', 'clip/text_model/final_layer_norm/gamma:0', 'clip/text_model/encoder/layers_._1/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._11/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._6/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._3/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._8/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._0/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._8/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._7/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._11/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._7/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._2/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._7/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._7/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._9/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._11/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._9/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._2/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._10/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._5/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._6/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._2/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._3/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._6/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._0/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._8/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._0/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._3/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._10/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._0/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._2/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._0/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._11/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._7/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._10/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._11/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._11/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._4/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._8/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._11/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._11/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._3/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._9/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._5/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._6/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._2/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._10/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._10/layer_norm2/beta:0', 'clip/logit_scale:0', 'clip/text_model/encoder/layers_._4/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._4/self_attn/out_proj/bias:0', 'clip/text_model/embeddings/position_embedding/embeddings:0', 'clip/text_model/encoder/layers_._9/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._8/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._3/self_attn/k_proj/bias:0', 'clip/visual_projection/kernel:0', 'clip/text_model/encoder/layers_._4/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._8/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._0/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._1/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._6/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._8/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._6/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._7/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._3/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._0/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._5/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._8/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._9/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._9/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._9/self_attn/v_proj/bias:0', 'clip/text_model/embeddings/token_embedding/weight:0', 'clip/text_model/encoder/layers_._11/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._3/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._1/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._2/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._2/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._0/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._3/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._2/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._10/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._10/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._10/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._1/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._9/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._4/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._6/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._4/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._4/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._1/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._7/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._4/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._9/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._9/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._8/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._2/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._6/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._8/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._0/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._5/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._11/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._1/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._9/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._0/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._5/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._10/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._4/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._3/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._8/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._7/self_attn/q_proj/kernel:0']\n",
            "- This IS expected if you are initializing TFCLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFCLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFCLIPVisionModel were initialized from the model checkpoint at openai/clip-vit-large-patch14-336.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPVisionModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "train_shard_suffix1 = '*.tfrec'\n",
        "train_shard_suffix2 = '*/*.tfrec'\n",
        "\n",
        "list_preproc_files_embeddings_train = []\n",
        "list_preproc_files_labels_train = []\n",
        "list_preproc_files_embeddings_valid = []\n",
        "list_preproc_files_labels_valid = []\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=config.SEED)\n",
        "\n",
        "previous_last_label = -1\n",
        "for (dataset_name, dataset_header) in dict_target_dataset.items():\n",
        "    GCS_DS_PATH = dict_target_dataset[ dataset_name ]\n",
        "\n",
        "    print( \"// ---------------------------------------\" )\n",
        "    print( f\"\\\"{dataset_name}\\\" : \\\"{GCS_DS_PATH}\\\",\" )\n",
        "    files1 = sorted(tf.io.gfile.glob(GCS_DS_PATH + f'/{train_shard_suffix1}'))\n",
        "    files2 = sorted(tf.io.gfile.glob(GCS_DS_PATH + f'/{train_shard_suffix2}'))\n",
        "    files = sorted(files1 + files2)\n",
        "    data_len = sum( [ get_num_of_image(file) for file in files ] )\n",
        "    print(files)\n",
        "    print(dataset_name, \", number of tfrecords = \", len(files), \"data length = \", data_len)\n",
        "\n",
        "    ds = get_backbone_inference_dataset(files,repeat = True)\n",
        "    print(ds)\n",
        "    steps = data_len // config.BATCH_SIZE_INFER\n",
        "    if data_len % config.BATCH_SIZE_INFER != 0:\n",
        "        steps += 1\n",
        "    backbone_output_array, label_array = backbone_model.predict( ds, verbose = 1, steps=steps)\n",
        "\n",
        "    # relabeling\n",
        "    label_array = label_array.astype(np.int64)\n",
        "    min_label = label_array.min()\n",
        "    max_label = label_array.max()\n",
        "    start_label = previous_last_label+1\n",
        "    end_label = previous_last_label+1+max_label-min_label\n",
        "\n",
        "    label_array = label_array - min_label + previous_last_label+1\n",
        "\n",
        "    # split data\n",
        "    #for train_index, valid_index in skf.split(range(label_array.shape[0]), label_array):\n",
        "    #    X_train, X_valid = backbone_output_array[train_index], backbone_output_array[valid_index]\n",
        "    #    y_train, y_valid = label_array[train_index], label_array[valid_index]    \n",
        "    #    break\n",
        "    X_train, X_valid = backbone_output_array, backbone_output_array\n",
        "    y_train, y_valid = label_array, label_array\n",
        "        \n",
        "    # save npy files\n",
        "    list_preproc_files_embeddings_train.append( f\"{PREPROC_DATASET_DIR}/_label{start_label}_{end_label}_train_embeddings.npy\" )\n",
        "    list_preproc_files_labels_train.append( f\"{PREPROC_DATASET_DIR}/_label{start_label}_{end_label}_train_labels.npy\" )\n",
        "    np.save( list_preproc_files_embeddings_train[-1], X_train)\n",
        "    np.save( list_preproc_files_labels_train[-1], y_train.astype(np.int64))\n",
        "\n",
        "    list_preproc_files_embeddings_valid.append( f\"{PREPROC_DATASET_DIR}/_label{start_label}_{end_label}_valid_embeddings.npy\" )\n",
        "    list_preproc_files_labels_valid.append( f\"{PREPROC_DATASET_DIR}/_label{start_label}_{end_label}_valid_labels.npy\" )\n",
        "    np.save( list_preproc_files_embeddings_valid[-1], X_valid)\n",
        "    np.save( list_preproc_files_labels_valid[-1], y_valid.astype(np.int64))\n",
        "\n",
        "    # update previous_last_label\n",
        "    previous_last_label = end_label\n",
        "\n",
        "print(f\"total_label =\", previous_last_label + 1)\n",
        "\n",
        "if config.N_CLASSES != previous_last_label + 1:\n",
        "    config.N_CLASSES = previous_last_label + 1\n",
        "    if config.N_CLASSES%64!=0:\n",
        "      config.N_CLASSES = config.N_CLASSES + 64 - config.N_CLASSES%64\n",
        "    print( \"!!! re-define model !!!\" )\n",
        "    del model\n",
        "    del emb_model\n",
        "    del backbone_model\n",
        "    del projection_model\n",
        "    gc.collect()\n",
        "    with strategy.scope():\n",
        "        model, emb_model, backbone_model, projection_model= get_embedding_model_new()\n",
        "\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate = config.LR)\n",
        "        projection_model.compile(\n",
        "            optimizer = opt,\n",
        "            loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
        "            metrics = [tf.keras.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n",
        "            )\n",
        "\n",
        "        if config.RESUME:\n",
        "            print(f\"load {config.RESUME_WEIGHT}\")\n",
        "            model.load_weights( config.RESUME_WEIGHT )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Kpb5HbAqzrxv",
        "outputId": "0fd9060d-e3a3-4ae1-a181-264a211562e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=({'inp1': TensorSpec(shape=(None, 336, 336, 3), dtype=tf.float32, name=None), 'inp2': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a_xj_uVJ7CbV",
        "outputId": "26b76f30-23ba-4e3f-c01d-3d7549bceb79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "22464"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config.N_CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Pt9vCZRw68ix",
        "outputId": "c6a3ccc6-fb75-42f7-d157-f3933c9ffdee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at openai/clip-vit-large-patch14-336 were not used when initializing TFCLIPVisionModel: ['clip/text_model/encoder/layers_._11/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._1/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._11/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._6/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._5/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._5/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._4/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._7/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._10/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._5/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._1/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._4/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._10/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._8/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._0/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._10/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._6/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._0/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._7/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._0/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._6/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._4/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._7/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._8/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._7/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._10/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._0/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._1/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._4/self_attn/v_proj/bias:0', 'clip/text_model/final_layer_norm/beta:0', 'clip/text_model/encoder/layers_._5/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._6/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._3/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._5/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._5/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._10/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._7/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._2/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._3/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._11/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._2/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._1/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._1/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._4/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._7/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._2/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._9/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._0/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._1/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._5/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._9/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._11/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._6/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._9/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._5/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._8/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._5/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._1/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._1/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._11/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._6/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._6/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._7/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._1/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._3/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._6/self_attn/q_proj/kernel:0', 'clip/text_projection/kernel:0', 'clip/text_model/encoder/layers_._9/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._2/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._3/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._10/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._1/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._4/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._4/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._2/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._2/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._5/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._0/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._8/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._7/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._5/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._3/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._10/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._9/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._8/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._11/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._3/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._3/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._2/layer_norm2/gamma:0', 'clip/text_model/final_layer_norm/gamma:0', 'clip/text_model/encoder/layers_._1/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._11/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._6/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._3/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._8/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._0/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._8/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._7/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._11/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._7/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._2/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._7/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._7/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._9/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._11/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._9/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._2/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._10/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._5/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._6/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._2/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._3/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._6/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._0/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._8/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._0/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._3/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._10/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._0/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._2/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._0/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._11/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._7/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._10/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._11/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._11/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._4/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._8/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._11/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._11/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._3/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._9/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._5/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._6/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._2/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._10/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._10/layer_norm2/beta:0', 'clip/logit_scale:0', 'clip/text_model/encoder/layers_._4/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._4/self_attn/out_proj/bias:0', 'clip/text_model/embeddings/position_embedding/embeddings:0', 'clip/text_model/encoder/layers_._9/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._8/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._3/self_attn/k_proj/bias:0', 'clip/visual_projection/kernel:0', 'clip/text_model/encoder/layers_._4/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._8/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._0/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._1/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._6/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._8/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._6/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._7/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._3/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._0/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._5/layer_norm1/gamma:0', 'clip/text_model/encoder/layers_._8/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._9/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._9/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._9/self_attn/v_proj/bias:0', 'clip/text_model/embeddings/token_embedding/weight:0', 'clip/text_model/encoder/layers_._11/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._3/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._1/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._2/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._2/self_attn/out_proj/bias:0', 'clip/text_model/encoder/layers_._0/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._3/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._2/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._10/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._10/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._10/mlp/fc2/bias:0', 'clip/text_model/encoder/layers_._1/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._9/self_attn/q_proj/bias:0', 'clip/text_model/encoder/layers_._4/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._6/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._4/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._4/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._1/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._7/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._4/layer_norm2/beta:0', 'clip/text_model/encoder/layers_._9/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._9/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._8/layer_norm1/beta:0', 'clip/text_model/encoder/layers_._2/self_attn/k_proj/bias:0', 'clip/text_model/encoder/layers_._6/mlp/fc1/bias:0', 'clip/text_model/encoder/layers_._8/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._0/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._5/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._11/self_attn/out_proj/kernel:0', 'clip/text_model/encoder/layers_._1/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._9/layer_norm2/gamma:0', 'clip/text_model/encoder/layers_._0/self_attn/q_proj/kernel:0', 'clip/text_model/encoder/layers_._5/self_attn/k_proj/kernel:0', 'clip/text_model/encoder/layers_._10/self_attn/v_proj/kernel:0', 'clip/text_model/encoder/layers_._4/mlp/fc2/kernel:0', 'clip/text_model/encoder/layers_._3/mlp/fc1/kernel:0', 'clip/text_model/encoder/layers_._8/self_attn/v_proj/bias:0', 'clip/text_model/encoder/layers_._7/self_attn/q_proj/kernel:0']\n",
            "- This IS expected if you are initializing TFCLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFCLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFCLIPVisionModel were initialized from the model checkpoint at openai/clip-vit-large-patch14-336.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPVisionModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "    model, emb_model, backbone_model, projection_model= get_embedding_model_new()\n",
        "\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate = config.LR)\n",
        "    projection_model.compile(\n",
        "        optimizer = opt,\n",
        "        loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
        "        metrics = [tf.keras.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n",
        "        )\n",
        "\n",
        "    if config.RESUME:\n",
        "        print(f\"load {config.RESUME_WEIGHT}\")\n",
        "        model.load_weights( config.RESUME_WEIGHT )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQMrFf_aYyAA"
      },
      "source": [
        "# Step2) Train projection model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ue_O5ADTY08o"
      },
      "outputs": [],
      "source": [
        "def arcface_format_projection(emb, label):\n",
        "    label = tf.cast( label, tf.int64 )\n",
        "    return {'inp_proj': emb, 'inp2': label}, label\n",
        "\n",
        "def get_projection_dataset_from_numpy(list_emb_npy, list_label_npy, cache=False, repeat=False, shuffle=False, augment=False):\n",
        "    datasets = []\n",
        "    total_len = 0\n",
        "    for (file1, file2) in zip( tqdm(list_emb_npy), list_label_npy ):\n",
        "        embeddings = np.load(file1)\n",
        "        labels     = np.load(file2).astype(np.int)\n",
        "        print(f\"{file1}: embeddings.shape={embeddings.shape}, labels.shape={labels.shape} label=[{labels.min()}, {labels.max()}]\" )\n",
        "        total_len += embeddings.shape[0]\n",
        "        dataset = tf.data.Dataset.from_tensor_slices( (embeddings, labels) )\n",
        "        datasets.append(dataset)\n",
        "    \n",
        "    dataset = datasets[0]\n",
        "    for tmp_ds in datasets[1:]:\n",
        "        dataset = dataset.concatenate( tmp_ds )\n",
        "    dataset = dataset.shuffle( total_len ) if shuffle else dataset\n",
        "    dataset = dataset.map(arcface_format_projection, num_parallel_calls=AUTO)\n",
        "    if repeat:\n",
        "        dataset = dataset.repeat()\n",
        "    dataset = dataset.batch(config.BATCH_SIZE_TRAIN)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset, total_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8HeWgBaHdKhs",
        "outputId": "8aa131d4-6712-46db-ac0d-105eca86dfda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lr_max 0.004\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.LearningRateScheduler at 0x7fa6439a90d0>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_lr_callback(plot=False):\n",
        "    lr_start   = 0.000001\n",
        "    lr_max     = 0.000005 * config.BATCH_SIZE_TRAIN  \n",
        "    print(\"lr_max\", lr_max)\n",
        "    lr_min     = 0.000001\n",
        "    lr_ramp_ep = 4\n",
        "    lr_sus_ep  = 0\n",
        "    lr_decay   = 0.95\n",
        "   \n",
        "    def lrfn(epoch):\n",
        "        if config.RESUME:\n",
        "            epoch = epoch + config.RESUME_EPOCH\n",
        "        if epoch < lr_ramp_ep:\n",
        "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
        "            \n",
        "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
        "            lr = lr_max\n",
        "            \n",
        "        else:\n",
        "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
        "            \n",
        "        return lr\n",
        "        \n",
        "    if plot:\n",
        "        epochs = list(range(config.EPOCHS))\n",
        "        learning_rates = [lrfn(x) for x in epochs]\n",
        "        plt.scatter(epochs,learning_rates)\n",
        "        plt.show()\n",
        "\n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
        "    return lr_callback\n",
        "\n",
        "get_lr_callback(plot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "3694b0f89779446f91f97ca1dc72f181",
            "b7853ea42de74887b6981f8747b6ad86"
          ]
        },
        "id": "914QlynRYnB_",
        "outputId": "f5e0ed96-9e8c-4450-e4fc-af0decc13b23"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3694b0f89779446f91f97ca1dc72f181",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./preproc//_label0_9690_train_embeddings.npy: embeddings.shape=(141952, 1024), labels.shape=(141952,) label=[0, 9690]\n",
            "./preproc//_label9691_16690_train_embeddings.npy: embeddings.shape=(349184, 1024), labels.shape=(349184,) label=[9691, 16690]\n",
            "./preproc//_label16691_22415_train_embeddings.npy: embeddings.shape=(385408, 1024), labels.shape=(385408,) label=[16691, 22415]\n",
            "876544\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7853ea42de74887b6981f8747b6ad86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./preproc//_label0_9690_valid_embeddings.npy: embeddings.shape=(141952, 1024), labels.shape=(141952,) label=[0, 9690]\n",
            "./preproc//_label9691_16690_valid_embeddings.npy: embeddings.shape=(349184, 1024), labels.shape=(349184,) label=[9691, 16690]\n",
            "./preproc//_label16691_22415_valid_embeddings.npy: embeddings.shape=(385408, 1024), labels.shape=(385408,) label=[16691, 22415]\n",
            "876544\n",
            "1096 1096\n"
          ]
        }
      ],
      "source": [
        "# callback to save weights\n",
        "sv_loss = tf.keras.callbacks.ModelCheckpoint(\n",
        "    config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_projection.h5\", monitor='val_sparse_categorical_accuracy', verbose=1, save_best_only=True,\n",
        "    save_weights_only=True, mode='max', save_freq='epoch')\n",
        "\n",
        "# dataset\n",
        "ds_train, train_len = get_projection_dataset_from_numpy( list_preproc_files_embeddings_train, list_preproc_files_labels_train, repeat=True, shuffle=True)\n",
        "print( train_len )\n",
        "\n",
        "ds_valid, valid_len = get_projection_dataset_from_numpy( list_preproc_files_embeddings_valid, list_preproc_files_labels_valid, repeat=False, shuffle=False)\n",
        "print( valid_len )\n",
        "\n",
        "# calc steps of dataset\n",
        "steps_per_epoch = train_len // config.BATCH_SIZE_TRAIN\n",
        "if train_len % config.BATCH_SIZE_TRAIN != 0:\n",
        "    steps_per_epoch += 1\n",
        "\n",
        "validation_steps = valid_len // config.BATCH_SIZE_TRAIN\n",
        "if valid_len % config.BATCH_SIZE_TRAIN != 0:\n",
        "    validation_steps += 1\n",
        "    \n",
        "print(steps_per_epoch, validation_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsGXfP7yI19x"
      },
      "outputs": [],
      "source": [
        "# training projection model\n",
        "if config.TRAIN:\n",
        "    history = projection_model.fit(\n",
        "        ds_train,\n",
        "        epochs=config.EPOCHS,\n",
        "        callbacks=[get_lr_callback(), sv_loss],\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data = ds_valid,\n",
        "        validation_steps = validation_steps,\n",
        "        verbose=1\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BIS5UoYfmwg"
      },
      "source": [
        "# Step3) save model weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mmwIebmPbi5B"
      },
      "outputs": [],
      "source": [
        "# load best weight \n",
        "projection_model.load_weights( config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_projection.h5\" )\n",
        "\n",
        "# save weight of entire model\n",
        "model.save_weights(  config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_loss.h5\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4WD0Qr2Q08HK"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/MyDrive/kaggle/google_embedding')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yXmhuLVfmwg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}