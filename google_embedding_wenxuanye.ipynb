{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5nKcowWfmwY"
      },
      "source": [
        "## I share the know-how to speed up training.\n",
        "\n",
        "## Archtecture\n",
        "<!-- This is a overview of model.  \n",
        "![model](https://github.com/motono0223/kaggle_public/blob/main/2022_guie/GUIE_Model.png?raw=true)   -->\n",
        "\n",
        "Training the entire model takes a long time because the backbone model is very large.  \n",
        "In this notebook, I propose to split model into a backbone model and a projection model like following steps.\n",
        "\n",
        "## Steps\n",
        "- Step1: infers the backbone model to get training data.  \n",
        "<!-- ![backbonemobel](https://github.com/motono0223/kaggle_public/blob/main/2022_guie/GUIE_BackboneModel.png?raw=true) -->\n",
        "\n",
        "- Step2: trains the projection model with output of step1.  \n",
        "<!-- ![projectionmodel](https://github.com/motono0223/kaggle_public/blob/main/2022_guie/GUIE_ProjectionModel.png?raw=true) -->\n",
        "\n",
        "- Step3: save weights of entire \"model\".  \n",
        "In this notebook, the entire \"model\" shares the same weights with backbone model and projection model.\n",
        "\n",
        "<!-- ## Note\n",
        "If you want to use the data augment method for training model, please consider to add the process of training \"model\" above as finetuning. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpNeCZR3y4Df"
      },
      "source": [
        "# Train in Colan enviroment \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i72153AaDJds",
        "outputId": "ab13715a-8f90-4261-e859-ca9c78b2dea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.18.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "# !pip install git+https://github.com/innat/transformers -U -q > /dev/null\n",
        "import os\n",
        "def is_colab_env():\n",
        "    is_colab = False\n",
        "    for k in os.environ.keys():\n",
        "        if \"COLAB\" in k:\n",
        "            is_colab = True\n",
        "            break\n",
        "    return is_colab\n",
        "    \n",
        "if is_colab_env():\n",
        "    !pip install transformers\n",
        "    !pip install tensorflow_addons\n",
        "from transformers import CLIPProcessor, TFCLIPVisionModel, CLIPFeatureExtractor\n",
        "\n",
        "# from kaggle_datasets import KaggleDatasets\n",
        "\n",
        "import re\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import normalize\n",
        "import pickle\n",
        "import json\n",
        "import tensorflow_hub as tfhub\n",
        "from datetime import datetime\n",
        "import gc\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "from mpl_toolkits import axes_grid1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KRxrCfQzGyv"
      },
      "source": [
        "# Make sure to use the TPU of Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khrTPhLcR39a",
        "outputId": "7cb45a83-6167-4ab3-fc04-a2a058e39a7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  grpc://10.55.240.34:8470\n",
            "REPLICAS:  8\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    # set: this is always the case on Kaggle.\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTsbyF_0zNu8"
      },
      "source": [
        "# Set the contant config value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCwQB_L1NGoH",
        "outputId": "5c22885d-e3c9-4c69-98a9-84ad633074f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clip-vit-large-patch14\n"
          ]
        }
      ],
      "source": [
        "class config:\n",
        "    VERSION = 60\n",
        "\n",
        "    SEED = 42\n",
        "    RESUME = False\n",
        "    RESUME_EPOCH = 0\n",
        "    RESUME_WEIGHT = \"\"\n",
        "\n",
        "    model_type = \"clip-vit-large-patch14\"\n",
        "    EFF_SIZE = 0\n",
        "    EFF2_TYPE = \"\"\n",
        "    # 336/224\n",
        "    IMAGE_SIZE = 224\n",
        "    BATCH_SIZE_INFER = 16 * strategy.num_replicas_in_sync\n",
        "    BATCH_SIZE_TRAIN = 100 * strategy.num_replicas_in_sync\n",
        "    N_CLASSES = 9691\n",
        "    EMB_DIM = 64\n",
        "    EPOCHS = 100\n",
        "    LR = 0.0012\n",
        "\n",
        "    save_dir = \"./\"\n",
        "\n",
        "    TRAIN = True\n",
        "    \n",
        "    DEBUG = False\n",
        "    \n",
        "    TTA = 1\n",
        "\n",
        "\n",
        "# Function to seed everything\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "seed_everything(config.SEED)\n",
        "\n",
        "MODEL_NAME = None\n",
        "if config.model_type == 'effnetv1':\n",
        "    MODEL_NAME = f'effnetv1_b{config.EFF_SIZE}'\n",
        "elif config.model_type == 'effnetv2':\n",
        "    MODEL_NAME = f'effnetv2_{config.EFF2_TYPE}'\n",
        "elif \"swin\" in config.model_type:\n",
        "    MODEL_NAME = config.model_type\n",
        "elif \"conv\" in config.model_type:\n",
        "    MODEL_NAME = config.model_type\n",
        "else:\n",
        "    MODEL_NAME = config.model_type\n",
        "\n",
        "config.MODEL_NAME = MODEL_NAME\n",
        "print(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDERhBqAzS1w"
      },
      "source": [
        "# The root path of our datasets in Google cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Yu0W1G3ddQz3"
      },
      "outputs": [],
      "source": [
        "dict_target_dataset = {\n",
        "    # base\n",
        "        # \"guie-imagenet1k-mini1-tfrecords-label-0-999\" : \"gs://wenxuanye/imagenet1k\",\n",
        "        \"guie-products10k-tfrecords-label-1000-10690\" : \"gs://wenxuanye/products10k\",\n",
        "        \"guie-glr2021mini-tfrecords-label-10691-17690\" : \"gs://wenxuanye/google_landmark\",\n",
        "        \"omnibench-label-17691\":\"gs://wenxuanye/omnibenchmark\",\n",
        "        # \"deepfashion\":\"gs://wenxuanye/deepfashion\"\n",
        "\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "PREPROC_DATASET_DIR = f\"./preproc/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZuGi10XOuiW"
      },
      "source": [
        "# Read the tfrecord datasets for classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hAmYgnXmFE3P"
      },
      "outputs": [],
      "source": [
        "def deserialization_fn(serialized_example):\n",
        "    parsed_example = tf.io.parse_single_example(\n",
        "        serialized_example,\n",
        "        features={\n",
        "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
        "            'image/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
        "        }\n",
        "    )\n",
        "    image = tf.image.decode_jpeg(parsed_example['image/encoded'], channels=3)\n",
        "    image = tf.image.resize(image, size=(config.IMAGE_SIZE, config.IMAGE_SIZE))\n",
        "    label = tf.cast(parsed_example['image/class/label'], tf.int64)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "def arcface_format(image, label_group):\n",
        "    return {'inp1': image, 'inp2': label_group}, label_group\n",
        "\n",
        "def rescale_image(image, label_group):\n",
        "    image = tf.cast(image, tf.float32) * 255.0\n",
        "    return image, label_group\n",
        "\n",
        "# Data augmentation function\n",
        "def data_augment(image, label_group):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    #image = tf.image.random_flip_up_down(image)\n",
        "    image = tf.image.random_hue(image, 0.01)\n",
        "    image = tf.image.random_saturation(image, 0.70, 1.30)\n",
        "    image = tf.image.random_contrast(image, 0.80, 1.20)\n",
        "    image = tf.image.random_brightness(image, 0.10)\n",
        "\n",
        "    return image, label_group\n",
        "\n",
        "# Dataset to obtain backbone's inference\n",
        "# output : ( image, label ), (label) \n",
        "def get_backbone_inference_dataset(tfrecord_paths, cache=False, repeat=False, shuffle=False, augment=False):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(tfrecord_paths)\n",
        "    dataset = dataset.shuffle(len(tfrecord_paths)) if shuffle else dataset\n",
        "    dataset = dataset.flat_map(tf.data.TFRecordDataset)\n",
        "    dataset = dataset.map(deserialization_fn, num_parallel_calls=AUTO) # image[0-1], label[0-999]\n",
        "\n",
        "    if augment:\n",
        "        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)  # (image, label_group) --> (image, label_group)\n",
        "    dataset = dataset.map(rescale_image, num_parallel_calls = AUTO)  # image[0-1], label[0-n_classes] --> image[0-255], label[0-n_classes]\n",
        "    dataset = dataset.map(arcface_format, num_parallel_calls=AUTO)   # (image, label_group) --> ({\"inp1\":image, \"inp2\":label_group}, label_group )\n",
        "    if repeat:\n",
        "        dataset = dataset.repeat()\n",
        "    dataset = dataset.batch(config.BATCH_SIZE_INFER)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOPX4LshNXsM"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG13FRxnfmwd"
      },
      "source": [
        "## Distance Margin Layer\n",
        "This notebook uses distance margin layer instead of the ArcFace Layer.  \n",
        "  \n",
        "With ArcFace, the embedding vectors are distributed over the surface of an N-dimensional sphere.   \n",
        "This is because ArcFaceLayer outputs the inner product of its own weight matrix and embedding vector to the next Softmax layer.   \n",
        "Although this embedding vector is easy to work with, it is not a very efficient use of the embedding vector space.  \n",
        "\n",
        "So I introduce a distance margin layer.   \n",
        "This outputs the inverse squared Euclidean distance in the next Softmax layer, not the inner product.  \n",
        "![DistanceMarginLayer](https://github.com/motono0223/kaggle_public/blob/main/2022_guie/GUIE_DistanceLayer.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SuwssBrY7qaU"
      },
      "outputs": [],
      "source": [
        "class DistanceMarginLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_classes, s=30, m=0.10, easy_margin=False,\n",
        "                 ls_eps=0.0, sgm=1.0, **kwargs):\n",
        "\n",
        "        super(DistanceMarginLayer, self).__init__(**kwargs)\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.ls_eps = ls_eps\n",
        "        self.easy_margin = easy_margin\n",
        "        self.sgm = sgm\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'n_classes': self.n_classes,\n",
        "            's': self.s,\n",
        "            'm': self.m,\n",
        "            'ls_eps': self.ls_eps,\n",
        "            'easy_margin': self.easy_margin,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(DistanceMarginLayer, self).build(input_shape[0])\n",
        "\n",
        "        self.W = self.add_weight(\n",
        "            name='W',\n",
        "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
        "            initializer='glorot_uniform',\n",
        "            dtype='float32',\n",
        "            trainable=True,\n",
        "            regularizer=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X, y = inputs\n",
        "        y = tf.cast(y, dtype=tf.int32)\n",
        "        \n",
        "        W2 = self.W \n",
        "\n",
        "        X2 = tf.tile(tf.expand_dims(X, 2), [1, 1, self.W.shape[1] ])  # X.shape=[Batch, EmbDim], X2.shape=[Batch, EmbDim, NumClass]\n",
        "\n",
        "        # distance on the N-Dimentional coordinate\n",
        "        dx = tf.math.sqrt( tf.reduce_sum( tf.math.pow( X2 - W2, 2 ), axis=1) ) # dx.shape=[Batch, NumClass]\n",
        "\n",
        "        dx = tf.clip_by_value( dx    , 0.00001, 50.0)\n",
        "        output1 = self.s / tf.math.pow( dx, 2 )\n",
        "        output2 = self.s / tf.math.pow( dx, 2 ) + self.m\n",
        "\n",
        "        one_hot = tf.cast(\n",
        "            tf.one_hot(y, depth=self.n_classes),\n",
        "            dtype=dx.dtype\n",
        "        )\n",
        "\n",
        "        output = (one_hot * output1) + ((1.0 - one_hot) * output2)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Arcmarginproduct class keras layer\n",
        "class ArcMarginProduct(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Implements large margin arc distance.\n",
        "\n",
        "    Reference:\n",
        "        https://arxiv.org/pdf/1801.07698.pdf\n",
        "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
        "            blob/master/src/modeling/metric_learning.py\n",
        "    '''\n",
        "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
        "                 ls_eps=0.0, **kwargs):\n",
        "\n",
        "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.ls_eps = ls_eps\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = tf.math.cos(m)\n",
        "        self.sin_m = tf.math.sin(m)\n",
        "        self.th = tf.math.cos(math.pi - m)\n",
        "        self.mm = tf.math.sin(math.pi - m) * m\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'n_classes': self.n_classes,\n",
        "            's': self.s,\n",
        "            'm': self.m,\n",
        "            'ls_eps': self.ls_eps,\n",
        "            'easy_margin': self.easy_margin,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ArcMarginProduct, self).build(input_shape[0])\n",
        "\n",
        "        self.W = self.add_weight(\n",
        "            name='W',\n",
        "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
        "            initializer='glorot_uniform',\n",
        "            dtype='float32',\n",
        "            trainable=True,\n",
        "            regularizer=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X, y = inputs\n",
        "        y = tf.cast(y, dtype=tf.int32)\n",
        "        cosine = tf.matmul(\n",
        "            tf.math.l2_normalize(X, axis=1),\n",
        "            tf.math.l2_normalize(self.W, axis=0)\n",
        "        )\n",
        "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = tf.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        one_hot = tf.cast(\n",
        "            tf.one_hot(y, depth=self.n_classes),\n",
        "            dtype=cosine.dtype\n",
        "        )\n",
        "        if self.ls_eps > 0:\n",
        "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
        "\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        return output\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jX76WJYoMgey"
      },
      "outputs": [],
      "source": [
        "def get_scale_layer(rescale_mode = \"tf\"):\n",
        "    # For keras_cv_attention_models module:\n",
        "    # ref: https://github.com/leondgarse/keras_cv_attention_models/blob/main/keras_cv_attention_models/imagenet/data.py\n",
        "    # ref function : init_mean_std_by_rescale_mode()\n",
        "\n",
        "    # For effV2 (21k classes) : https://github.com/leondgarse/keras_efficientnet_v2\n",
        "\n",
        "    if isinstance(rescale_mode, (list, tuple)):  # Specific mean and std\n",
        "        mean, std = rescale_mode\n",
        "    elif rescale_mode == \"torch\":\n",
        "        mean = np.array([0.485, 0.456, 0.406]) * 255.0\n",
        "        std = np.array([0.229, 0.224, 0.225]) * 255.0\n",
        "    elif rescale_mode == \"tf\":  # [0, 255] -> [-1, 1]\n",
        "        mean, std = 127.5, 127.5\n",
        "    elif rescale_mode == \"tf128\":  # [0, 255] -> [-1, 1]\n",
        "        mean, std = 128.0, 128.0\n",
        "    elif rescale_mode == \"raw01\":\n",
        "        mean, std = 0, 255.0  # [0, 255] -> [0, 1]\n",
        "    else:\n",
        "        mean, std = 0, 1  # raw inputs [0, 255]        \n",
        "    scaling_layer = keras.layers.Lambda(lambda x: ( tf.cast(x, tf.float32) - mean) / std )\n",
        "    \n",
        "    return scaling_layer\n",
        "\n",
        "def get_clip_model():\n",
        "    inp = tf.keras.layers.Input(shape = [3, config.IMAGE_SIZE, config.IMAGE_SIZE])\n",
        "#    backbone = TFCLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "# openai/clip-vit-large-patch14-336\n",
        "# openai/clip-vit-large-patch14\n",
        "    backbone = TFCLIPVisionModel.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\",from_pt = True)\n",
        "    output = backbone({'pixel_values':inp}).pooler_output\n",
        "    return tf.keras.Model(inputs=[inp], outputs=[output])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uASDSuOFHQ1G"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_embedding_model_new(frozen=True):\n",
        "    #------------------\n",
        "    # Definition of placeholders\n",
        "    inp = tf.keras.layers.Input(shape = [None, None, 3], name = 'inp1')\n",
        "    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
        "\n",
        "    # Definition of layers\n",
        "    layer_resize = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, [config.IMAGE_SIZE, config.IMAGE_SIZE]), name='resize')\n",
        "    layer_scaling = get_scale_layer(rescale_mode = \"torch\")\n",
        "    layer_permute = tf.keras.layers.Permute((3,1,2))\n",
        "    layer_backbone = get_clip_model()\n",
        "    if frozen:\n",
        "        layer_backbone.trainable = False\n",
        "    # layer_pool = tf.keras.layers.GlobalAveragePooling1D(1024)\n",
        "    # add 1d batch norm\n",
        "    layer_batch_norm = tf.keras.layers.BatchNormalization()\n",
        "    # add prreLU\n",
        "    layer_prelu = tf.keras.layers.PReLU()\n",
        "    layer_dropout = tf.keras.layers.Dropout(0.2)\n",
        "    layer_dense_before_arcface = tf.keras.layers.Dense(256)\n",
        "    layer_margin = DistanceMarginLayer(\n",
        "        n_classes = config.N_CLASSES, \n",
        "        s = 30, \n",
        "        m = 0.3,  # no margin in this notebook\n",
        "        name=f'head/distancemargin', \n",
        "        dtype='float32'\n",
        "        )\n",
        "    # layer_margin = ArcMarginProduct(\n",
        "    #     n_classes = config.N_CLASSES, \n",
        "    #     s = 30, \n",
        "    #     m = 0.3, \n",
        "    #     name=f'head/arcface', \n",
        "    #     dtype='float32'\n",
        "    #     )\n",
        "\n",
        "\n",
        "    layer_softmax = tf.keras.layers.Softmax(dtype='float32')\n",
        "    layer_adaptive_pooling = tfa.layers.AdaptiveAveragePooling1D(64)\n",
        "    layer_norm = tf.keras.layers.Lambda(lambda x: x, name='embedding_norm')\n",
        "    #------------------\n",
        "    # Definition of entire model \n",
        "    image = layer_scaling(inp)\n",
        "    image = layer_resize(image)\n",
        "    image = layer_permute(image)\n",
        "    backbone_output = layer_backbone(image)\n",
        "    # embed = layer_pool(backbone_output)\n",
        "    embed = layer_dropout(backbone_output)\n",
        "    embed = layer_batch_norm(embed)\n",
        "    embed = layer_prelu(embed)\n",
        "    \n",
        "    embed = layer_dense_before_arcface(embed)\n",
        "    x = layer_margin([embed, label])\n",
        "    output = layer_softmax(x)\n",
        "    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output]) # whole architecture\n",
        "\n",
        "    #------------------\n",
        "    # Definition of embedding model (for submission)\n",
        "    x = layer_adaptive_pooling(embed)\n",
        "    emboutput = layer_norm(x)\n",
        "    embed_model = tf.keras.models.Model(inputs = [inp, label], outputs = [emboutput]) # whole architecture\n",
        "    \n",
        "    #------------------\n",
        "    # Definition of backbone model (to obtain training data)\n",
        "    backbone_model = tf.keras.models.Model(inputs = [inp, label], outputs = [backbone_output, label])  \n",
        "\n",
        "    #------------------\n",
        "    # Definition of projection Model (to train projection layers)\n",
        "    # print(backbone_output.shape)\n",
        "    inp_proj = tf.keras.layers.Input(shape = [backbone_output.shape[-1]], name = 'inp_proj')\n",
        "    x = layer_dropout(inp_proj)\n",
        "    x = layer_batch_norm(x)\n",
        "    x = layer_prelu(x)\n",
        "    x = layer_dense_before_arcface(x)\n",
        "    x = layer_margin([x, label])\n",
        "    output = layer_softmax(x)\n",
        "    projection_model = tf.keras.models.Model(inputs = [inp_proj, label], outputs = [output])\n",
        "\n",
        "    # Note: These 4 models share the same weights each other.\n",
        "    return model, embed_model, backbone_model, projection_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nGM8XncMglt",
        "outputId": "3a6c4c04-0637-42d0-8c67-f67b7dafd102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFCLIPVisionModel: ['text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.21.self_attn.v_proj.bias', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.19.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.18.mlp.fc2.bias', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'logit_scale', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_projection.weight', 'visual_projection.weight', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.22.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.19.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.15.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.15.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.embeddings.position_ids', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight']\n",
            "- This IS expected if you are initializing TFCLIPVisionModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFCLIPVisionModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFCLIPVisionModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPVisionModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "    model, emb_model, backbone_model, projection_model = get_embedding_model_new()\n",
        "\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate = config.LR)\n",
        "    projection_model.compile(\n",
        "        optimizer = opt,\n",
        "        loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
        "        metrics = [tf.keras.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n",
        "        )\n",
        "\n",
        "    if config.RESUME:\n",
        "        print(f\"load {config.RESUME_WEIGHT}\")\n",
        "        model.load_weights( config.RESUME_WEIGHT )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSWkPesHMgpO",
        "outputId": "f51e5dc6-0406-495c-cfa5-6d80363fcea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inp1 (InputLayer)              [(None, None, None,  0           []                               \n",
            "                                 3)]                                                              \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, None, None,   0           ['inp1[0][0]']                   \n",
            "                                3)                                                                \n",
            "                                                                                                  \n",
            " resize (Lambda)                (None, 224, 224, 3)  0           ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " permute (Permute)              (None, 3, 224, 224)  0           ['resize[0][0]']                 \n",
            "                                                                                                  \n",
            " model (Functional)             (None, 1280)         630766080   ['permute[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_32 (Dropout)           (None, 1280)         0           ['model[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 1280)        5120        ['dropout_32[0][0]']             \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " p_re_lu (PReLU)                (None, 1280)         1280        ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 256)          327936      ['p_re_lu[0][0]']                \n",
            "                                                                                                  \n",
            " inp2 (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " head/distancemargin (DistanceM  (None, 9691)        2480896     ['dense[0][0]',                  \n",
            " arginLayer)                                                      'inp2[0][0]']                   \n",
            "                                                                                                  \n",
            " softmax (Softmax)              (None, 9691)         0           ['head/distancemargin[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 633,581,312\n",
            "Trainable params: 2,812,672\n",
            "Non-trainable params: 630,768,640\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a5LW0tnMgsX",
        "outputId": "183ef379-7368-4df3-bdff-a2bc20a3e651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inp1 (InputLayer)              [(None, None, None,  0           []                               \n",
            "                                 3)]                                                              \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, None, None,   0           ['inp1[0][0]']                   \n",
            "                                3)                                                                \n",
            "                                                                                                  \n",
            " resize (Lambda)                (None, 224, 224, 3)  0           ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " permute (Permute)              (None, 3, 224, 224)  0           ['resize[0][0]']                 \n",
            "                                                                                                  \n",
            " model (Functional)             (None, 1280)         630766080   ['permute[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_32 (Dropout)           (None, 1280)         0           ['model[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 1280)        5120        ['dropout_32[0][0]']             \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " p_re_lu (PReLU)                (None, 1280)         1280        ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 256)          327936      ['p_re_lu[0][0]']                \n",
            "                                                                                                  \n",
            " adaptive_average_pooling1d (Ad  (None, 64)          0           ['dense[0][0]']                  \n",
            " aptiveAveragePooling1D)                                                                          \n",
            "                                                                                                  \n",
            " inp2 (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " embedding_norm (Lambda)        (None, 64)           0           ['adaptive_average_pooling1d[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 631,100,416\n",
            "Trainable params: 331,776\n",
            "Non-trainable params: 630,768,640\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "emb_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_etfk-hrDwGn",
        "outputId": "68576d0b-55b2-496b-d1a9-b3bf927c171c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inp1 (InputLayer)              [(None, None, None,  0           []                               \n",
            "                                 3)]                                                              \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, None, None,   0           ['inp1[0][0]']                   \n",
            "                                3)                                                                \n",
            "                                                                                                  \n",
            " resize (Lambda)                (None, 224, 224, 3)  0           ['lambda[0][0]']                 \n",
            "                                                                                                  \n",
            " permute (Permute)              (None, 3, 224, 224)  0           ['resize[0][0]']                 \n",
            "                                                                                                  \n",
            " model (Functional)             (None, 1280)         630766080   ['permute[0][0]']                \n",
            "                                                                                                  \n",
            " inp2 (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 630,766,080\n",
            "Trainable params: 0\n",
            "Non-trainable params: 630,766,080\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "backbone_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3iDqpUHIn5H",
        "outputId": "e7b9395e-fc8b-4b86-a77e-ca67034acb59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inp_proj (InputLayer)          [(None, 1280)]       0           []                               \n",
            "                                                                                                  \n",
            " dropout_32 (Dropout)           (None, 1280)         0           ['inp_proj[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 1280)        5120        ['dropout_32[1][0]']             \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " p_re_lu (PReLU)                (None, 1280)         1280        ['batch_normalization[1][0]']    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 256)          327936      ['p_re_lu[1][0]']                \n",
            "                                                                                                  \n",
            " inp2 (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " head/distancemargin (DistanceM  (None, 9691)        2480896     ['dense[1][0]',                  \n",
            " arginLayer)                                                      'inp2[0][0]']                   \n",
            "                                                                                                  \n",
            " softmax (Softmax)              (None, 9691)         0           ['head/distancemargin[1][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,815,232\n",
            "Trainable params: 2,812,672\n",
            "Non-trainable params: 2,560\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "projection_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4YZEydFrI4m"
      },
      "source": [
        "# Step1) Inference of backbone model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "V5UPgaI-ry2M"
      },
      "outputs": [],
      "source": [
        "def get_num_of_image(file):\n",
        "    return int(file.split(\"/\")[-1].split(\".\")[0].split(\"-\")[-1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECod4QZErIF_",
        "outputId": "4704728c-1ebc-418e-91be-126894a11e76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "// ---------------------------------------\n",
            "\"guie-products10k-tfrecords-label-1000-10690\" : \"gs://wenxuanye/products10k\",\n",
            "['gs://wenxuanye/products10k/guie-products10k-train-00-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-01-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-02-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-03-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-04-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-05-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-06-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-07-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-08-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-09-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-10-7097.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-11-7096.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-12-7096.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-13-7096.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-14-7096.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-15-7096.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-16-7096.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-17-7096.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-18-7096.tfrec', 'gs://wenxuanye/products10k/guie-products10k-train-19-7096.tfrec']\n",
            "guie-products10k-tfrecords-label-1000-10690 , number of tfrecords =  20 data length =  141931\n",
            "1109/1109 [==============================] - 492s 435ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "// ---------------------------------------\n",
            "\"guie-glr2021mini-tfrecords-label-10691-17690\" : \"gs://wenxuanye/google_landmark\",\n",
            "['gs://wenxuanye/google_landmark/guie-glr2021-train-00-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-01-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-02-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-03-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-04-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-05-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-06-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-07-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-08-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-09-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-10-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-11-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-12-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-13-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-14-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-15-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-16-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-17-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-18-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-19-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-20-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-21-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-22-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-23-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-24-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-25-10909.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-26-10908.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-27-10908.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-28-10908.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-29-10908.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-30-10908.tfrec', 'gs://wenxuanye/google_landmark/guie-glr2021-train-31-10908.tfrec']\n",
            "guie-glr2021mini-tfrecords-label-10691-17690 , number of tfrecords =  32 data length =  349082\n",
            "2728/2728 [==============================] - 1147s 421ms/step\n",
            "// ---------------------------------------\n",
            "\"omnibench-label-17691\" : \"gs://wenxuanye/omnibenchmark\",\n",
            "['gs://wenxuanye/omnibenchmark/guie-omniv1-train-00-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-01-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-02-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-03-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-04-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-05-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-06-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-07-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-08-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-09-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-10-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-11-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-12-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-13-19268.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-14-19267.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-15-19267.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-16-19267.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-17-19267.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-18-19267.tfrec', 'gs://wenxuanye/omnibenchmark/guie-omniv1-train-19-19267.tfrec']\n",
            "omnibench-label-17691 , number of tfrecords =  20 data length =  385354\n",
            "3011/3011 [==============================] - 1264s 420ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_label = 22416\n",
            "!!! re-define model !!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFCLIPVisionModel: ['text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.21.self_attn.v_proj.bias', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.19.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.18.mlp.fc2.bias', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'logit_scale', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_projection.weight', 'visual_projection.weight', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.22.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.19.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.15.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.15.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.embeddings.position_ids', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight']\n",
            "- This IS expected if you are initializing TFCLIPVisionModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFCLIPVisionModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFCLIPVisionModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPVisionModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "train_shard_suffix1 = '*.tfrec'\n",
        "train_shard_suffix2 = '*/*.tfrec'\n",
        "\n",
        "list_preproc_files_embeddings_train = []\n",
        "list_preproc_files_labels_train = []\n",
        "list_preproc_files_embeddings_valid = []\n",
        "list_preproc_files_labels_valid = []\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=config.SEED)\n",
        "\n",
        "previous_last_label = -1\n",
        "for (dataset_name, dataset_header) in dict_target_dataset.items():\n",
        "    GCS_DS_PATH = dict_target_dataset[ dataset_name ]\n",
        "\n",
        "    print( \"// ---------------------------------------\" )\n",
        "    print( f\"\\\"{dataset_name}\\\" : \\\"{GCS_DS_PATH}\\\",\" )\n",
        "    files1 = sorted(tf.io.gfile.glob(GCS_DS_PATH + f'/{train_shard_suffix1}'))\n",
        "    files2 = sorted(tf.io.gfile.glob(GCS_DS_PATH + f'/{train_shard_suffix2}'))\n",
        "    files = sorted(files1 + files2)\n",
        "    data_len = sum( [ get_num_of_image(file) for file in files ] )\n",
        "    print(files)\n",
        "    print(dataset_name, \", number of tfrecords = \", len(files), \"data length = \", data_len)\n",
        "\n",
        "    ds = get_backbone_inference_dataset(files,repeat = True)\n",
        "    # print(ds)\n",
        "    steps = data_len // config.BATCH_SIZE_INFER\n",
        "    if data_len % config.BATCH_SIZE_INFER != 0:\n",
        "        steps += 1\n",
        "    backbone_output_array, label_array = backbone_model.predict( ds, verbose = 1, steps=steps)\n",
        "\n",
        "    # relabeling\n",
        "    label_array = label_array.astype(np.int64)\n",
        "    min_label = label_array.min()\n",
        "    max_label = label_array.max()\n",
        "    start_label = previous_last_label+1\n",
        "    end_label = previous_last_label+1+max_label-min_label\n",
        "\n",
        "    label_array = label_array - min_label + previous_last_label+1\n",
        "\n",
        "    # split data\n",
        "    for train_index, valid_index in skf.split(range(label_array.shape[0]), label_array):\n",
        "       X_train, X_valid = backbone_output_array[train_index], backbone_output_array[valid_index]\n",
        "       y_train, y_valid = label_array[train_index], label_array[valid_index]    \n",
        "       break\n",
        "    # X_train, X_valid = backbone_output_array, backbone_output_array\n",
        "    # y_train, y_valid = label_array, label_array\n",
        "        \n",
        "    # save npy files\n",
        "    list_preproc_files_embeddings_train.append( f\"{PREPROC_DATASET_DIR}/_label{start_label}_{end_label}_train_embeddings.npy\" )\n",
        "    list_preproc_files_labels_train.append( f\"{PREPROC_DATASET_DIR}/_label{start_label}_{end_label}_train_labels.npy\" )\n",
        "    np.save( list_preproc_files_embeddings_train[-1], X_train)\n",
        "    np.save( list_preproc_files_labels_train[-1], y_train.astype(np.int64))\n",
        "\n",
        "    list_preproc_files_embeddings_valid.append( f\"{PREPROC_DATASET_DIR}/_label{start_label}_{end_label}_valid_embeddings.npy\" )\n",
        "    list_preproc_files_labels_valid.append( f\"{PREPROC_DATASET_DIR}/_label{start_label}_{end_label}_valid_labels.npy\" )\n",
        "    np.save( list_preproc_files_embeddings_valid[-1], X_valid)\n",
        "    np.save( list_preproc_files_labels_valid[-1], y_valid.astype(np.int64))\n",
        "\n",
        "    # update previous_last_label\n",
        "    previous_last_label = end_label\n",
        "\n",
        "print(f\"total_label =\", previous_last_label + 1)\n",
        "\n",
        "if config.N_CLASSES != previous_last_label + 1:\n",
        "    config.N_CLASSES = previous_last_label + 1\n",
        "    print( \"!!! re-define model !!!\" )\n",
        "    del model\n",
        "    del emb_model\n",
        "    del backbone_model\n",
        "    del projection_model\n",
        "    gc.collect()\n",
        "    with strategy.scope():\n",
        "        model, emb_model, backbone_model, projection_model= get_embedding_model_new()\n",
        "\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate = config.LR)\n",
        "        projection_model.compile(\n",
        "            optimizer = opt,\n",
        "            loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
        "            metrics = [tf.keras.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n",
        "            )\n",
        "\n",
        "        if config.RESUME:\n",
        "            print(f\"load {config.RESUME_WEIGHT}\")\n",
        "            model.load_weights( config.RESUME_WEIGHT )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pt9vCZRw68ix",
        "outputId": "6c9d533f-cc24-4dec-e43a-d3fc6cb54c6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFCLIPVisionModel: ['text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.21.self_attn.v_proj.bias', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.19.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.18.mlp.fc2.bias', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'logit_scale', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_projection.weight', 'visual_projection.weight', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.22.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.19.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.15.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.15.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.embeddings.position_ids', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight']\n",
            "- This IS expected if you are initializing TFCLIPVisionModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFCLIPVisionModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFCLIPVisionModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPVisionModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "    model, emb_model, backbone_model, projection_model= get_embedding_model_new()\n",
        "\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate = config.LR)\n",
        "    projection_model.compile(\n",
        "        optimizer = opt,\n",
        "        loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
        "        metrics = [tf.keras.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n",
        "        )\n",
        "\n",
        "    if config.RESUME:\n",
        "        print(f\"load {config.RESUME_WEIGHT}\")\n",
        "        model.load_weights( config.RESUME_WEIGHT )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQMrFf_aYyAA"
      },
      "source": [
        "# Step2) Train projection model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ue_O5ADTY08o"
      },
      "outputs": [],
      "source": [
        "def arcface_format_projection(emb, label):\n",
        "    label = tf.cast( label, tf.int64 )\n",
        "    return {'inp_proj': emb, 'inp2': label}, label\n",
        "\n",
        "def get_projection_dataset_from_numpy(list_emb_npy, list_label_npy, cache=False, repeat=False, shuffle=False, augment=False):\n",
        "    datasets = []\n",
        "    total_len = 0\n",
        "    for (file1, file2) in zip( tqdm(list_emb_npy), list_label_npy ):\n",
        "        embeddings = np.load(file1)\n",
        "        labels     = np.load(file2).astype(np.int)\n",
        "        print(f\"{file1}: embeddings.shape={embeddings.shape}, labels.shape={labels.shape} label=[{labels.min()}, {labels.max()}]\" )\n",
        "        total_len += embeddings.shape[0]\n",
        "        dataset = tf.data.Dataset.from_tensor_slices( (embeddings, labels) )\n",
        "        datasets.append(dataset)\n",
        "    \n",
        "    dataset = datasets[0]\n",
        "    for tmp_ds in datasets[1:]:\n",
        "        dataset = dataset.concatenate( tmp_ds )\n",
        "    dataset = dataset.shuffle( total_len ) if shuffle else dataset\n",
        "    dataset = dataset.map(arcface_format_projection, num_parallel_calls=AUTO)\n",
        "    if repeat:\n",
        "        dataset = dataset.repeat()\n",
        "    dataset = dataset.batch(config.BATCH_SIZE_TRAIN)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset, total_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "8HeWgBaHdKhs",
        "outputId": "c41d624b-ab3c-48e2-feef-f0bc55c1f515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr_max 0.004\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.LearningRateScheduler at 0x7f5bab2fedd0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "def get_lr_callback(plot=False):\n",
        "    lr_start   = 0.000001\n",
        "    lr_max     = 0.000005 * config.BATCH_SIZE_TRAIN  \n",
        "    print(\"lr_max\", lr_max)\n",
        "    lr_min     = 0.000001\n",
        "    lr_ramp_ep = 4\n",
        "    lr_sus_ep  = 0\n",
        "    lr_decay   = 0.95\n",
        "   \n",
        "    def lrfn(epoch):\n",
        "        if config.RESUME:\n",
        "            epoch = epoch + config.RESUME_EPOCH\n",
        "        if epoch < lr_ramp_ep:\n",
        "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
        "            \n",
        "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
        "            lr = lr_max\n",
        "            \n",
        "        else:\n",
        "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
        "            \n",
        "        return lr\n",
        "        \n",
        "    if plot:\n",
        "        epochs = list(range(config.EPOCHS))\n",
        "        learning_rates = [lrfn(x) for x in epochs]\n",
        "        plt.scatter(epochs,learning_rates)\n",
        "        plt.show()\n",
        "\n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
        "    return lr_callback\n",
        "\n",
        "get_lr_callback(plot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6649c6bd186a436cb29bb41fd90e9351",
            "93ef3de69b9c466988166becd2531c27",
            "e07f56085d40449b824a30183099caeb",
            "b568725f98e745548498aa6e8bd2bd06",
            "a821bfd397364f7abbae147873f11be8",
            "04a88c3c501247b1a2b8966e32414d59",
            "610cd412ec434f4d85b90a6e91aab412",
            "cb0d12faf09441158dca42e905f32ba0",
            "67340a73358c45df8cd0b947cff96c2b",
            "f29d5e26e0d34a91985dc8b504941ba7",
            "7073ba683b1146ea9f259cf292b19768",
            "e2981bea196146c8b728b78902c4bfc5",
            "7472720330be426c804c42c1cd8a1782",
            "def52a89e7e74724b1c916dfb3d06324",
            "0a355dcc8a49424da5532ed1a7725eaf",
            "76ca37987a8d4b5d96fc380579f3d73c",
            "7c4cb3c567f04d94822e696a7ef2ecc7",
            "593f2961c59b4528b1387e44779184cd",
            "08538074df7c4535aaa523fcb1cf8612",
            "ee8359e474df4d8bb47799752958e999",
            "d6b9bbf32f2e46cb984f1aa8842f60f0",
            "cd53548d4dbc41e5933498c58a9fcf0b"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "914QlynRYnB_",
        "outputId": "813a62d3-1497-42ae-eece-347fa9bda982"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6649c6bd186a436cb29bb41fd90e9351"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./preproc//_label0_9690_train_embeddings.npy: embeddings.shape=(127756, 1280), labels.shape=(127756,) label=[0, 9690]\n",
            "./preproc//_label9691_16690_train_embeddings.npy: embeddings.shape=(314265, 1280), labels.shape=(314265,) label=[9691, 16690]\n",
            "./preproc//_label16691_22415_train_embeddings.npy: embeddings.shape=(346867, 1280), labels.shape=(346867,) label=[16691, 22415]\n",
            "788888\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2981bea196146c8b728b78902c4bfc5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./preproc//_label0_9690_valid_embeddings.npy: embeddings.shape=(14196, 1280), labels.shape=(14196,) label=[0, 9688]\n",
            "./preproc//_label9691_16690_valid_embeddings.npy: embeddings.shape=(34919, 1280), labels.shape=(34919,) label=[9691, 16690]\n",
            "./preproc//_label16691_22415_valid_embeddings.npy: embeddings.shape=(38541, 1280), labels.shape=(38541,) label=[16691, 22415]\n",
            "87656\n",
            "987 110\n"
          ]
        }
      ],
      "source": [
        "# callback to save weights\n",
        "sv_loss = tf.keras.callbacks.ModelCheckpoint(\n",
        "    config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_projection.h5\", monitor='val_sparse_categorical_accuracy', verbose=1, save_best_only=True,\n",
        "    save_weights_only=True, mode='max', save_freq='epoch')\n",
        "\n",
        "# dataset\n",
        "ds_train, train_len = get_projection_dataset_from_numpy( list_preproc_files_embeddings_train, list_preproc_files_labels_train, repeat=True, shuffle=True)\n",
        "print( train_len )\n",
        "\n",
        "ds_valid, valid_len = get_projection_dataset_from_numpy( list_preproc_files_embeddings_valid, list_preproc_files_labels_valid, repeat=False, shuffle=False)\n",
        "print( valid_len )\n",
        "\n",
        "# calc steps of dataset\n",
        "steps_per_epoch = train_len // config.BATCH_SIZE_TRAIN\n",
        "if train_len % config.BATCH_SIZE_TRAIN != 0:\n",
        "    steps_per_epoch += 1\n",
        "\n",
        "validation_steps = valid_len // config.BATCH_SIZE_TRAIN\n",
        "if valid_len % config.BATCH_SIZE_TRAIN != 0:\n",
        "    validation_steps += 1\n",
        "    \n",
        "print(steps_per_epoch, validation_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LsGXfP7yI19x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94e41def-e518-44e6-9b67-b1d224c5c6b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr_max 0.004\n",
            "Epoch 1/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 10.3175 - sparse_categorical_accuracy: 0.0000e+00 - sparse_top_k_categorical_accuracy: 0.0000e+00\n",
            "Epoch 1: val_sparse_categorical_accuracy improved from -inf to 0.00000, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 46s 38ms/step - loss: 10.3175 - sparse_categorical_accuracy: 0.0000e+00 - sparse_top_k_categorical_accuracy: 0.0000e+00 - val_loss: 10.3175 - val_sparse_categorical_accuracy: 0.0000e+00 - val_sparse_top_k_categorical_accuracy: 0.0000e+00 - lr: 1.0000e-06\n",
            "Epoch 2/100\n",
            "986/987 [============================>.] - ETA: 0s - loss: 4.2202 - sparse_categorical_accuracy: 0.3638 - sparse_top_k_categorical_accuracy: 0.5583\n",
            "Epoch 2: val_sparse_categorical_accuracy improved from 0.00000 to 0.52001, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 4.2179 - sparse_categorical_accuracy: 0.3640 - sparse_top_k_categorical_accuracy: 0.5586 - val_loss: 2.5170 - val_sparse_categorical_accuracy: 0.5200 - val_sparse_top_k_categorical_accuracy: 0.7470 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 2.3668 - sparse_categorical_accuracy: 0.5404 - sparse_top_k_categorical_accuracy: 0.7635\n",
            "Epoch 3: val_sparse_categorical_accuracy improved from 0.52001 to 0.53598, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 2.3668 - sparse_categorical_accuracy: 0.5404 - sparse_top_k_categorical_accuracy: 0.7635 - val_loss: 2.4570 - val_sparse_categorical_accuracy: 0.5360 - val_sparse_top_k_categorical_accuracy: 0.7626 - lr: 0.0020\n",
            "Epoch 4/100\n",
            "986/987 [============================>.] - ETA: 0s - loss: 2.2956 - sparse_categorical_accuracy: 0.5545 - sparse_top_k_categorical_accuracy: 0.7767\n",
            "Epoch 4: val_sparse_categorical_accuracy improved from 0.53598 to 0.54275, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 2.2952 - sparse_categorical_accuracy: 0.5546 - sparse_top_k_categorical_accuracy: 0.7767 - val_loss: 2.4771 - val_sparse_categorical_accuracy: 0.5427 - val_sparse_top_k_categorical_accuracy: 0.7646 - lr: 0.0030\n",
            "Epoch 5/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 2.4214 - sparse_categorical_accuracy: 0.5425 - sparse_top_k_categorical_accuracy: 0.7635\n",
            "Epoch 5: val_sparse_categorical_accuracy did not improve from 0.54275\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 2.4214 - sparse_categorical_accuracy: 0.5425 - sparse_top_k_categorical_accuracy: 0.7635 - val_loss: 2.5789 - val_sparse_categorical_accuracy: 0.5324 - val_sparse_top_k_categorical_accuracy: 0.7494 - lr: 0.0040\n",
            "Epoch 6/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 2.3257 - sparse_categorical_accuracy: 0.5573 - sparse_top_k_categorical_accuracy: 0.7755\n",
            "Epoch 6: val_sparse_categorical_accuracy improved from 0.54275 to 0.54287, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 2.3257 - sparse_categorical_accuracy: 0.5573 - sparse_top_k_categorical_accuracy: 0.7755 - val_loss: 2.4831 - val_sparse_categorical_accuracy: 0.5429 - val_sparse_top_k_categorical_accuracy: 0.7605 - lr: 0.0038\n",
            "Epoch 7/100\n",
            "986/987 [============================>.] - ETA: 0s - loss: 2.2535 - sparse_categorical_accuracy: 0.5662 - sparse_top_k_categorical_accuracy: 0.7838\n",
            "Epoch 7: val_sparse_categorical_accuracy improved from 0.54287 to 0.55090, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 44s 45ms/step - loss: 2.2532 - sparse_categorical_accuracy: 0.5662 - sparse_top_k_categorical_accuracy: 0.7839 - val_loss: 2.4096 - val_sparse_categorical_accuracy: 0.5509 - val_sparse_top_k_categorical_accuracy: 0.7686 - lr: 0.0036\n",
            "Epoch 8/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 2.1950 - sparse_categorical_accuracy: 0.5743 - sparse_top_k_categorical_accuracy: 0.7905\n",
            "Epoch 8: val_sparse_categorical_accuracy improved from 0.55090 to 0.55704, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 39s 40ms/step - loss: 2.1950 - sparse_categorical_accuracy: 0.5743 - sparse_top_k_categorical_accuracy: 0.7905 - val_loss: 2.3639 - val_sparse_categorical_accuracy: 0.5570 - val_sparse_top_k_categorical_accuracy: 0.7733 - lr: 0.0034\n",
            "Epoch 9/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 2.1369 - sparse_categorical_accuracy: 0.5824 - sparse_top_k_categorical_accuracy: 0.7974\n",
            "Epoch 9: val_sparse_categorical_accuracy improved from 0.55704 to 0.56115, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 2.1369 - sparse_categorical_accuracy: 0.5824 - sparse_top_k_categorical_accuracy: 0.7974 - val_loss: 2.3068 - val_sparse_categorical_accuracy: 0.5611 - val_sparse_top_k_categorical_accuracy: 0.7790 - lr: 0.0033\n",
            "Epoch 10/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 2.0766 - sparse_categorical_accuracy: 0.5900 - sparse_top_k_categorical_accuracy: 0.8038\n",
            "Epoch 10: val_sparse_categorical_accuracy improved from 0.56115 to 0.57011, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 2.0766 - sparse_categorical_accuracy: 0.5900 - sparse_top_k_categorical_accuracy: 0.8038 - val_loss: 2.2814 - val_sparse_categorical_accuracy: 0.5701 - val_sparse_top_k_categorical_accuracy: 0.7819 - lr: 0.0031\n",
            "Epoch 11/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 2.0138 - sparse_categorical_accuracy: 0.5987 - sparse_top_k_categorical_accuracy: 0.8114\n",
            "Epoch 11: val_sparse_categorical_accuracy improved from 0.57011 to 0.57332, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 2.0138 - sparse_categorical_accuracy: 0.5987 - sparse_top_k_categorical_accuracy: 0.8114 - val_loss: 2.2102 - val_sparse_categorical_accuracy: 0.5733 - val_sparse_top_k_categorical_accuracy: 0.7894 - lr: 0.0029\n",
            "Epoch 12/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.9525 - sparse_categorical_accuracy: 0.6078 - sparse_top_k_categorical_accuracy: 0.8184\n",
            "Epoch 12: val_sparse_categorical_accuracy improved from 0.57332 to 0.58378, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 1.9525 - sparse_categorical_accuracy: 0.6078 - sparse_top_k_categorical_accuracy: 0.8184 - val_loss: 2.1527 - val_sparse_categorical_accuracy: 0.5838 - val_sparse_top_k_categorical_accuracy: 0.7958 - lr: 0.0028\n",
            "Epoch 13/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.8928 - sparse_categorical_accuracy: 0.6162 - sparse_top_k_categorical_accuracy: 0.8242\n",
            "Epoch 13: val_sparse_categorical_accuracy improved from 0.58378 to 0.58655, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 1.8928 - sparse_categorical_accuracy: 0.6162 - sparse_top_k_categorical_accuracy: 0.8242 - val_loss: 2.1168 - val_sparse_categorical_accuracy: 0.5866 - val_sparse_top_k_categorical_accuracy: 0.7999 - lr: 0.0027\n",
            "Epoch 14/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.8350 - sparse_categorical_accuracy: 0.6246 - sparse_top_k_categorical_accuracy: 0.8312\n",
            "Epoch 14: val_sparse_categorical_accuracy improved from 0.58655 to 0.59358, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 35s 35ms/step - loss: 1.8350 - sparse_categorical_accuracy: 0.6246 - sparse_top_k_categorical_accuracy: 0.8312 - val_loss: 2.0761 - val_sparse_categorical_accuracy: 0.5936 - val_sparse_top_k_categorical_accuracy: 0.8047 - lr: 0.0025\n",
            "Epoch 15/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.7796 - sparse_categorical_accuracy: 0.6336 - sparse_top_k_categorical_accuracy: 0.8376\n",
            "Epoch 15: val_sparse_categorical_accuracy improved from 0.59358 to 0.59586, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 1.7796 - sparse_categorical_accuracy: 0.6336 - sparse_top_k_categorical_accuracy: 0.8376 - val_loss: 2.0443 - val_sparse_categorical_accuracy: 0.5959 - val_sparse_top_k_categorical_accuracy: 0.8083 - lr: 0.0024\n",
            "Epoch 16/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.7215 - sparse_categorical_accuracy: 0.6421 - sparse_top_k_categorical_accuracy: 0.8435\n",
            "Epoch 16: val_sparse_categorical_accuracy improved from 0.59586 to 0.60778, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 1.7215 - sparse_categorical_accuracy: 0.6421 - sparse_top_k_categorical_accuracy: 0.8435 - val_loss: 1.9962 - val_sparse_categorical_accuracy: 0.6078 - val_sparse_top_k_categorical_accuracy: 0.8143 - lr: 0.0023\n",
            "Epoch 17/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.6701 - sparse_categorical_accuracy: 0.6499 - sparse_top_k_categorical_accuracy: 0.8498\n",
            "Epoch 17: val_sparse_categorical_accuracy did not improve from 0.60778\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 1.6701 - sparse_categorical_accuracy: 0.6499 - sparse_top_k_categorical_accuracy: 0.8498 - val_loss: 1.9956 - val_sparse_categorical_accuracy: 0.6043 - val_sparse_top_k_categorical_accuracy: 0.8152 - lr: 0.0022\n",
            "Epoch 18/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.6146 - sparse_categorical_accuracy: 0.6592 - sparse_top_k_categorical_accuracy: 0.8559\n",
            "Epoch 18: val_sparse_categorical_accuracy improved from 0.60778 to 0.60983, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 1.6146 - sparse_categorical_accuracy: 0.6592 - sparse_top_k_categorical_accuracy: 0.8559 - val_loss: 1.9553 - val_sparse_categorical_accuracy: 0.6098 - val_sparse_top_k_categorical_accuracy: 0.8194 - lr: 0.0021\n",
            "Epoch 19/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.5654 - sparse_categorical_accuracy: 0.6669 - sparse_top_k_categorical_accuracy: 0.8610\n",
            "Epoch 19: val_sparse_categorical_accuracy improved from 0.60983 to 0.61576, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 1.5654 - sparse_categorical_accuracy: 0.6669 - sparse_top_k_categorical_accuracy: 0.8610 - val_loss: 1.9224 - val_sparse_categorical_accuracy: 0.6158 - val_sparse_top_k_categorical_accuracy: 0.8219 - lr: 0.0020\n",
            "Epoch 20/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.5179 - sparse_categorical_accuracy: 0.6743 - sparse_top_k_categorical_accuracy: 0.8660\n",
            "Epoch 20: val_sparse_categorical_accuracy improved from 0.61576 to 0.61769, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 1.5179 - sparse_categorical_accuracy: 0.6743 - sparse_top_k_categorical_accuracy: 0.8660 - val_loss: 1.9033 - val_sparse_categorical_accuracy: 0.6177 - val_sparse_top_k_categorical_accuracy: 0.8256 - lr: 0.0019\n",
            "Epoch 21/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.4689 - sparse_categorical_accuracy: 0.6825 - sparse_top_k_categorical_accuracy: 0.8712\n",
            "Epoch 21: val_sparse_categorical_accuracy improved from 0.61769 to 0.62414, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 1.4689 - sparse_categorical_accuracy: 0.6825 - sparse_top_k_categorical_accuracy: 0.8712 - val_loss: 1.8591 - val_sparse_categorical_accuracy: 0.6241 - val_sparse_top_k_categorical_accuracy: 0.8285 - lr: 0.0018\n",
            "Epoch 22/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.4211 - sparse_categorical_accuracy: 0.6902 - sparse_top_k_categorical_accuracy: 0.8766\n",
            "Epoch 22: val_sparse_categorical_accuracy improved from 0.62414 to 0.62683, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 1.4211 - sparse_categorical_accuracy: 0.6902 - sparse_top_k_categorical_accuracy: 0.8766 - val_loss: 1.8391 - val_sparse_categorical_accuracy: 0.6268 - val_sparse_top_k_categorical_accuracy: 0.8323 - lr: 0.0017\n",
            "Epoch 23/100\n",
            "986/987 [============================>.] - ETA: 0s - loss: 1.3780 - sparse_categorical_accuracy: 0.6975 - sparse_top_k_categorical_accuracy: 0.8813\n",
            "Epoch 23: val_sparse_categorical_accuracy improved from 0.62683 to 0.63368, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 1.3776 - sparse_categorical_accuracy: 0.6975 - sparse_top_k_categorical_accuracy: 0.8814 - val_loss: 1.8039 - val_sparse_categorical_accuracy: 0.6337 - val_sparse_top_k_categorical_accuracy: 0.8353 - lr: 0.0016\n",
            "Epoch 24/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.3371 - sparse_categorical_accuracy: 0.7047 - sparse_top_k_categorical_accuracy: 0.8856\n",
            "Epoch 24: val_sparse_categorical_accuracy improved from 0.63368 to 0.63480, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 1.3371 - sparse_categorical_accuracy: 0.7047 - sparse_top_k_categorical_accuracy: 0.8856 - val_loss: 1.7886 - val_sparse_categorical_accuracy: 0.6348 - val_sparse_top_k_categorical_accuracy: 0.8383 - lr: 0.0015\n",
            "Epoch 25/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.2948 - sparse_categorical_accuracy: 0.7119 - sparse_top_k_categorical_accuracy: 0.8897\n",
            "Epoch 25: val_sparse_categorical_accuracy improved from 0.63480 to 0.63807, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 1.2948 - sparse_categorical_accuracy: 0.7119 - sparse_top_k_categorical_accuracy: 0.8897 - val_loss: 1.7739 - val_sparse_categorical_accuracy: 0.6381 - val_sparse_top_k_categorical_accuracy: 0.8395 - lr: 0.0014\n",
            "Epoch 26/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.2554 - sparse_categorical_accuracy: 0.7189 - sparse_top_k_categorical_accuracy: 0.8941\n",
            "Epoch 26: val_sparse_categorical_accuracy improved from 0.63807 to 0.63854, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 1.2554 - sparse_categorical_accuracy: 0.7189 - sparse_top_k_categorical_accuracy: 0.8941 - val_loss: 1.7606 - val_sparse_categorical_accuracy: 0.6385 - val_sparse_top_k_categorical_accuracy: 0.8417 - lr: 0.0014\n",
            "Epoch 27/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.2158 - sparse_categorical_accuracy: 0.7261 - sparse_top_k_categorical_accuracy: 0.8978\n",
            "Epoch 27: val_sparse_categorical_accuracy improved from 0.63854 to 0.64519, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 1.2158 - sparse_categorical_accuracy: 0.7261 - sparse_top_k_categorical_accuracy: 0.8978 - val_loss: 1.7304 - val_sparse_categorical_accuracy: 0.6452 - val_sparse_top_k_categorical_accuracy: 0.8452 - lr: 0.0013\n",
            "Epoch 28/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.1804 - sparse_categorical_accuracy: 0.7322 - sparse_top_k_categorical_accuracy: 0.9016\n",
            "Epoch 28: val_sparse_categorical_accuracy did not improve from 0.64519\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 1.1804 - sparse_categorical_accuracy: 0.7322 - sparse_top_k_categorical_accuracy: 0.9016 - val_loss: 1.7321 - val_sparse_categorical_accuracy: 0.6448 - val_sparse_top_k_categorical_accuracy: 0.8446 - lr: 0.0012\n",
            "Epoch 29/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.1442 - sparse_categorical_accuracy: 0.7388 - sparse_top_k_categorical_accuracy: 0.9051\n",
            "Epoch 29: val_sparse_categorical_accuracy improved from 0.64519 to 0.64839, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 1.1442 - sparse_categorical_accuracy: 0.7388 - sparse_top_k_categorical_accuracy: 0.9051 - val_loss: 1.7030 - val_sparse_categorical_accuracy: 0.6484 - val_sparse_top_k_categorical_accuracy: 0.8467 - lr: 0.0012\n",
            "Epoch 30/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.1092 - sparse_categorical_accuracy: 0.7454 - sparse_top_k_categorical_accuracy: 0.9093\n",
            "Epoch 30: val_sparse_categorical_accuracy improved from 0.64839 to 0.65098, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 1.1092 - sparse_categorical_accuracy: 0.7454 - sparse_top_k_categorical_accuracy: 0.9093 - val_loss: 1.6895 - val_sparse_categorical_accuracy: 0.6510 - val_sparse_top_k_categorical_accuracy: 0.8493 - lr: 0.0011\n",
            "Epoch 31/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.0757 - sparse_categorical_accuracy: 0.7513 - sparse_top_k_categorical_accuracy: 0.9123\n",
            "Epoch 31: val_sparse_categorical_accuracy improved from 0.65098 to 0.65278, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 1.0757 - sparse_categorical_accuracy: 0.7513 - sparse_top_k_categorical_accuracy: 0.9123 - val_loss: 1.6801 - val_sparse_categorical_accuracy: 0.6528 - val_sparse_top_k_categorical_accuracy: 0.8504 - lr: 0.0011\n",
            "Epoch 32/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.0453 - sparse_categorical_accuracy: 0.7578 - sparse_top_k_categorical_accuracy: 0.9154\n",
            "Epoch 32: val_sparse_categorical_accuracy improved from 0.65278 to 0.65598, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 1.0453 - sparse_categorical_accuracy: 0.7578 - sparse_top_k_categorical_accuracy: 0.9154 - val_loss: 1.6658 - val_sparse_categorical_accuracy: 0.6560 - val_sparse_top_k_categorical_accuracy: 0.8524 - lr: 0.0010\n",
            "Epoch 33/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 1.0151 - sparse_categorical_accuracy: 0.7631 - sparse_top_k_categorical_accuracy: 0.9183\n",
            "Epoch 33: val_sparse_categorical_accuracy improved from 0.65598 to 0.65759, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 1.0151 - sparse_categorical_accuracy: 0.7631 - sparse_top_k_categorical_accuracy: 0.9183 - val_loss: 1.6548 - val_sparse_categorical_accuracy: 0.6576 - val_sparse_top_k_categorical_accuracy: 0.8536 - lr: 9.5207e-04\n",
            "Epoch 34/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.9862 - sparse_categorical_accuracy: 0.7685 - sparse_top_k_categorical_accuracy: 0.9212\n",
            "Epoch 34: val_sparse_categorical_accuracy improved from 0.65759 to 0.65929, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.9862 - sparse_categorical_accuracy: 0.7685 - sparse_top_k_categorical_accuracy: 0.9212 - val_loss: 1.6483 - val_sparse_categorical_accuracy: 0.6593 - val_sparse_top_k_categorical_accuracy: 0.8550 - lr: 9.0452e-04\n",
            "Epoch 35/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.9570 - sparse_categorical_accuracy: 0.7737 - sparse_top_k_categorical_accuracy: 0.9243\n",
            "Epoch 35: val_sparse_categorical_accuracy improved from 0.65929 to 0.66256, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.9570 - sparse_categorical_accuracy: 0.7737 - sparse_top_k_categorical_accuracy: 0.9243 - val_loss: 1.6359 - val_sparse_categorical_accuracy: 0.6626 - val_sparse_top_k_categorical_accuracy: 0.8551 - lr: 8.5934e-04\n",
            "Epoch 36/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.9301 - sparse_categorical_accuracy: 0.7797 - sparse_top_k_categorical_accuracy: 0.9265\n",
            "Epoch 36: val_sparse_categorical_accuracy did not improve from 0.66256\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.9301 - sparse_categorical_accuracy: 0.7797 - sparse_top_k_categorical_accuracy: 0.9265 - val_loss: 1.6333 - val_sparse_categorical_accuracy: 0.6624 - val_sparse_top_k_categorical_accuracy: 0.8555 - lr: 8.1642e-04\n",
            "Epoch 37/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.9029 - sparse_categorical_accuracy: 0.7850 - sparse_top_k_categorical_accuracy: 0.9293\n",
            "Epoch 37: val_sparse_categorical_accuracy improved from 0.66256 to 0.66472, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.9029 - sparse_categorical_accuracy: 0.7850 - sparse_top_k_categorical_accuracy: 0.9293 - val_loss: 1.6174 - val_sparse_categorical_accuracy: 0.6647 - val_sparse_top_k_categorical_accuracy: 0.8580 - lr: 7.7565e-04\n",
            "Epoch 38/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.8777 - sparse_categorical_accuracy: 0.7902 - sparse_top_k_categorical_accuracy: 0.9317\n",
            "Epoch 38: val_sparse_categorical_accuracy improved from 0.66472 to 0.66626, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.8777 - sparse_categorical_accuracy: 0.7902 - sparse_top_k_categorical_accuracy: 0.9317 - val_loss: 1.6150 - val_sparse_categorical_accuracy: 0.6663 - val_sparse_top_k_categorical_accuracy: 0.8586 - lr: 7.3692e-04\n",
            "Epoch 39/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.8555 - sparse_categorical_accuracy: 0.7946 - sparse_top_k_categorical_accuracy: 0.9336\n",
            "Epoch 39: val_sparse_categorical_accuracy did not improve from 0.66626\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.8555 - sparse_categorical_accuracy: 0.7946 - sparse_top_k_categorical_accuracy: 0.9336 - val_loss: 1.6079 - val_sparse_categorical_accuracy: 0.6655 - val_sparse_top_k_categorical_accuracy: 0.8589 - lr: 7.0012e-04\n",
            "Epoch 40/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.8316 - sparse_categorical_accuracy: 0.7994 - sparse_top_k_categorical_accuracy: 0.9365\n",
            "Epoch 40: val_sparse_categorical_accuracy improved from 0.66626 to 0.66771, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.8316 - sparse_categorical_accuracy: 0.7994 - sparse_top_k_categorical_accuracy: 0.9365 - val_loss: 1.6071 - val_sparse_categorical_accuracy: 0.6677 - val_sparse_top_k_categorical_accuracy: 0.8604 - lr: 6.6517e-04\n",
            "Epoch 41/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.8110 - sparse_categorical_accuracy: 0.8035 - sparse_top_k_categorical_accuracy: 0.9383\n",
            "Epoch 41: val_sparse_categorical_accuracy improved from 0.66771 to 0.66957, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.8110 - sparse_categorical_accuracy: 0.8035 - sparse_top_k_categorical_accuracy: 0.9383 - val_loss: 1.5942 - val_sparse_categorical_accuracy: 0.6696 - val_sparse_top_k_categorical_accuracy: 0.8604 - lr: 6.3196e-04\n",
            "Epoch 42/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.7881 - sparse_categorical_accuracy: 0.8087 - sparse_top_k_categorical_accuracy: 0.9402\n",
            "Epoch 42: val_sparse_categorical_accuracy did not improve from 0.66957\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.7881 - sparse_categorical_accuracy: 0.8087 - sparse_top_k_categorical_accuracy: 0.9402 - val_loss: 1.5895 - val_sparse_categorical_accuracy: 0.6690 - val_sparse_top_k_categorical_accuracy: 0.8622 - lr: 6.0041e-04\n",
            "Epoch 43/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.7683 - sparse_categorical_accuracy: 0.8123 - sparse_top_k_categorical_accuracy: 0.9424\n",
            "Epoch 43: val_sparse_categorical_accuracy improved from 0.66957 to 0.67082, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.7683 - sparse_categorical_accuracy: 0.8123 - sparse_top_k_categorical_accuracy: 0.9424 - val_loss: 1.5939 - val_sparse_categorical_accuracy: 0.6708 - val_sparse_top_k_categorical_accuracy: 0.8615 - lr: 5.7044e-04\n",
            "Epoch 44/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.7499 - sparse_categorical_accuracy: 0.8159 - sparse_top_k_categorical_accuracy: 0.9441\n",
            "Epoch 44: val_sparse_categorical_accuracy improved from 0.67082 to 0.67166, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.7499 - sparse_categorical_accuracy: 0.8159 - sparse_top_k_categorical_accuracy: 0.9441 - val_loss: 1.5848 - val_sparse_categorical_accuracy: 0.6717 - val_sparse_top_k_categorical_accuracy: 0.8632 - lr: 5.4197e-04\n",
            "Epoch 45/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.7305 - sparse_categorical_accuracy: 0.8205 - sparse_top_k_categorical_accuracy: 0.9457\n",
            "Epoch 45: val_sparse_categorical_accuracy improved from 0.67166 to 0.67362, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 35s 35ms/step - loss: 0.7305 - sparse_categorical_accuracy: 0.8205 - sparse_top_k_categorical_accuracy: 0.9457 - val_loss: 1.5930 - val_sparse_categorical_accuracy: 0.6736 - val_sparse_top_k_categorical_accuracy: 0.8630 - lr: 5.1492e-04\n",
            "Epoch 46/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.7130 - sparse_categorical_accuracy: 0.8236 - sparse_top_k_categorical_accuracy: 0.9476\n",
            "Epoch 46: val_sparse_categorical_accuracy did not improve from 0.67362\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.7130 - sparse_categorical_accuracy: 0.8236 - sparse_top_k_categorical_accuracy: 0.9476 - val_loss: 1.5836 - val_sparse_categorical_accuracy: 0.6728 - val_sparse_top_k_categorical_accuracy: 0.8633 - lr: 4.8922e-04\n",
            "Epoch 47/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.6962 - sparse_categorical_accuracy: 0.8275 - sparse_top_k_categorical_accuracy: 0.9490\n",
            "Epoch 47: val_sparse_categorical_accuracy improved from 0.67362 to 0.67674, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.6962 - sparse_categorical_accuracy: 0.8275 - sparse_top_k_categorical_accuracy: 0.9490 - val_loss: 1.5790 - val_sparse_categorical_accuracy: 0.6767 - val_sparse_top_k_categorical_accuracy: 0.8651 - lr: 4.6481e-04\n",
            "Epoch 48/100\n",
            "986/987 [============================>.] - ETA: 0s - loss: 0.6806 - sparse_categorical_accuracy: 0.8305 - sparse_top_k_categorical_accuracy: 0.9505\n",
            "Epoch 48: val_sparse_categorical_accuracy did not improve from 0.67674\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.6805 - sparse_categorical_accuracy: 0.8305 - sparse_top_k_categorical_accuracy: 0.9505 - val_loss: 1.5804 - val_sparse_categorical_accuracy: 0.6758 - val_sparse_top_k_categorical_accuracy: 0.8646 - lr: 4.4162e-04\n",
            "Epoch 49/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.6656 - sparse_categorical_accuracy: 0.8338 - sparse_top_k_categorical_accuracy: 0.9520\n",
            "Epoch 49: val_sparse_categorical_accuracy did not improve from 0.67674\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.6656 - sparse_categorical_accuracy: 0.8338 - sparse_top_k_categorical_accuracy: 0.9520 - val_loss: 1.5817 - val_sparse_categorical_accuracy: 0.6759 - val_sparse_top_k_categorical_accuracy: 0.8652 - lr: 4.1959e-04\n",
            "Epoch 50/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.6497 - sparse_categorical_accuracy: 0.8370 - sparse_top_k_categorical_accuracy: 0.9533\n",
            "Epoch 50: val_sparse_categorical_accuracy improved from 0.67674 to 0.67695, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.6497 - sparse_categorical_accuracy: 0.8370 - sparse_top_k_categorical_accuracy: 0.9533 - val_loss: 1.5795 - val_sparse_categorical_accuracy: 0.6770 - val_sparse_top_k_categorical_accuracy: 0.8653 - lr: 3.9866e-04\n",
            "Epoch 51/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.6362 - sparse_categorical_accuracy: 0.8397 - sparse_top_k_categorical_accuracy: 0.9548\n",
            "Epoch 51: val_sparse_categorical_accuracy improved from 0.67695 to 0.67776, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.6362 - sparse_categorical_accuracy: 0.8397 - sparse_top_k_categorical_accuracy: 0.9548 - val_loss: 1.5775 - val_sparse_categorical_accuracy: 0.6778 - val_sparse_top_k_categorical_accuracy: 0.8661 - lr: 3.7878e-04\n",
            "Epoch 52/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.6236 - sparse_categorical_accuracy: 0.8428 - sparse_top_k_categorical_accuracy: 0.9558\n",
            "Epoch 52: val_sparse_categorical_accuracy improved from 0.67776 to 0.67848, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.6236 - sparse_categorical_accuracy: 0.8428 - sparse_top_k_categorical_accuracy: 0.9558 - val_loss: 1.5798 - val_sparse_categorical_accuracy: 0.6785 - val_sparse_top_k_categorical_accuracy: 0.8660 - lr: 3.5989e-04\n",
            "Epoch 53/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.6090 - sparse_categorical_accuracy: 0.8456 - sparse_top_k_categorical_accuracy: 0.9570\n",
            "Epoch 53: val_sparse_categorical_accuracy did not improve from 0.67848\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.6090 - sparse_categorical_accuracy: 0.8456 - sparse_top_k_categorical_accuracy: 0.9570 - val_loss: 1.5761 - val_sparse_categorical_accuracy: 0.6781 - val_sparse_top_k_categorical_accuracy: 0.8666 - lr: 3.4195e-04\n",
            "Epoch 54/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.5978 - sparse_categorical_accuracy: 0.8482 - sparse_top_k_categorical_accuracy: 0.9583\n",
            "Epoch 54: val_sparse_categorical_accuracy improved from 0.67848 to 0.67949, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.5978 - sparse_categorical_accuracy: 0.8482 - sparse_top_k_categorical_accuracy: 0.9583 - val_loss: 1.5739 - val_sparse_categorical_accuracy: 0.6795 - val_sparse_top_k_categorical_accuracy: 0.8662 - lr: 3.2490e-04\n",
            "Epoch 55/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.5852 - sparse_categorical_accuracy: 0.8508 - sparse_top_k_categorical_accuracy: 0.9593\n",
            "Epoch 55: val_sparse_categorical_accuracy improved from 0.67949 to 0.68085, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.5852 - sparse_categorical_accuracy: 0.8508 - sparse_top_k_categorical_accuracy: 0.9593 - val_loss: 1.5755 - val_sparse_categorical_accuracy: 0.6809 - val_sparse_top_k_categorical_accuracy: 0.8679 - lr: 3.0870e-04\n",
            "Epoch 56/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.5745 - sparse_categorical_accuracy: 0.8531 - sparse_top_k_categorical_accuracy: 0.9602\n",
            "Epoch 56: val_sparse_categorical_accuracy did not improve from 0.68085\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.5745 - sparse_categorical_accuracy: 0.8531 - sparse_top_k_categorical_accuracy: 0.9602 - val_loss: 1.5794 - val_sparse_categorical_accuracy: 0.6806 - val_sparse_top_k_categorical_accuracy: 0.8674 - lr: 2.9332e-04\n",
            "Epoch 57/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.5645 - sparse_categorical_accuracy: 0.8553 - sparse_top_k_categorical_accuracy: 0.9612\n",
            "Epoch 57: val_sparse_categorical_accuracy improved from 0.68085 to 0.68103, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.5645 - sparse_categorical_accuracy: 0.8553 - sparse_top_k_categorical_accuracy: 0.9612 - val_loss: 1.5797 - val_sparse_categorical_accuracy: 0.6810 - val_sparse_top_k_categorical_accuracy: 0.8675 - lr: 2.7870e-04\n",
            "Epoch 58/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.5527 - sparse_categorical_accuracy: 0.8582 - sparse_top_k_categorical_accuracy: 0.9623\n",
            "Epoch 58: val_sparse_categorical_accuracy improved from 0.68103 to 0.68195, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.5527 - sparse_categorical_accuracy: 0.8582 - sparse_top_k_categorical_accuracy: 0.9623 - val_loss: 1.5768 - val_sparse_categorical_accuracy: 0.6819 - val_sparse_top_k_categorical_accuracy: 0.8682 - lr: 2.6482e-04\n",
            "Epoch 59/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.5421 - sparse_categorical_accuracy: 0.8601 - sparse_top_k_categorical_accuracy: 0.9633\n",
            "Epoch 59: val_sparse_categorical_accuracy did not improve from 0.68195\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.5421 - sparse_categorical_accuracy: 0.8601 - sparse_top_k_categorical_accuracy: 0.9633 - val_loss: 1.5798 - val_sparse_categorical_accuracy: 0.6814 - val_sparse_top_k_categorical_accuracy: 0.8676 - lr: 2.5163e-04\n",
            "Epoch 60/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.5340 - sparse_categorical_accuracy: 0.8619 - sparse_top_k_categorical_accuracy: 0.9640\n",
            "Epoch 60: val_sparse_categorical_accuracy improved from 0.68195 to 0.68312, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.5340 - sparse_categorical_accuracy: 0.8619 - sparse_top_k_categorical_accuracy: 0.9640 - val_loss: 1.5792 - val_sparse_categorical_accuracy: 0.6831 - val_sparse_top_k_categorical_accuracy: 0.8683 - lr: 2.3909e-04\n",
            "Epoch 61/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.5253 - sparse_categorical_accuracy: 0.8639 - sparse_top_k_categorical_accuracy: 0.9649\n",
            "Epoch 61: val_sparse_categorical_accuracy did not improve from 0.68312\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.5253 - sparse_categorical_accuracy: 0.8639 - sparse_top_k_categorical_accuracy: 0.9649 - val_loss: 1.5764 - val_sparse_categorical_accuracy: 0.6817 - val_sparse_top_k_categorical_accuracy: 0.8686 - lr: 2.2719e-04\n",
            "Epoch 62/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.5166 - sparse_categorical_accuracy: 0.8658 - sparse_top_k_categorical_accuracy: 0.9657\n",
            "Epoch 62: val_sparse_categorical_accuracy improved from 0.68312 to 0.68325, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.5166 - sparse_categorical_accuracy: 0.8658 - sparse_top_k_categorical_accuracy: 0.9657 - val_loss: 1.5826 - val_sparse_categorical_accuracy: 0.6833 - val_sparse_top_k_categorical_accuracy: 0.8687 - lr: 2.1588e-04\n",
            "Epoch 63/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.5075 - sparse_categorical_accuracy: 0.8677 - sparse_top_k_categorical_accuracy: 0.9664\n",
            "Epoch 63: val_sparse_categorical_accuracy improved from 0.68325 to 0.68436, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.5075 - sparse_categorical_accuracy: 0.8677 - sparse_top_k_categorical_accuracy: 0.9664 - val_loss: 1.5794 - val_sparse_categorical_accuracy: 0.6844 - val_sparse_top_k_categorical_accuracy: 0.8688 - lr: 2.0514e-04\n",
            "Epoch 64/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.5013 - sparse_categorical_accuracy: 0.8694 - sparse_top_k_categorical_accuracy: 0.9670\n",
            "Epoch 64: val_sparse_categorical_accuracy did not improve from 0.68436\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.5013 - sparse_categorical_accuracy: 0.8694 - sparse_top_k_categorical_accuracy: 0.9670 - val_loss: 1.5834 - val_sparse_categorical_accuracy: 0.6834 - val_sparse_top_k_categorical_accuracy: 0.8689 - lr: 1.9493e-04\n",
            "Epoch 65/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4929 - sparse_categorical_accuracy: 0.8712 - sparse_top_k_categorical_accuracy: 0.9678\n",
            "Epoch 65: val_sparse_categorical_accuracy improved from 0.68436 to 0.68521, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.4929 - sparse_categorical_accuracy: 0.8712 - sparse_top_k_categorical_accuracy: 0.9678 - val_loss: 1.5834 - val_sparse_categorical_accuracy: 0.6852 - val_sparse_top_k_categorical_accuracy: 0.8683 - lr: 1.8523e-04\n",
            "Epoch 66/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4859 - sparse_categorical_accuracy: 0.8723 - sparse_top_k_categorical_accuracy: 0.9685\n",
            "Epoch 66: val_sparse_categorical_accuracy did not improve from 0.68521\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.4859 - sparse_categorical_accuracy: 0.8723 - sparse_top_k_categorical_accuracy: 0.9685 - val_loss: 1.5910 - val_sparse_categorical_accuracy: 0.6848 - val_sparse_top_k_categorical_accuracy: 0.8690 - lr: 1.7602e-04\n",
            "Epoch 67/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4794 - sparse_categorical_accuracy: 0.8741 - sparse_top_k_categorical_accuracy: 0.9691\n",
            "Epoch 67: val_sparse_categorical_accuracy did not improve from 0.68521\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.4794 - sparse_categorical_accuracy: 0.8741 - sparse_top_k_categorical_accuracy: 0.9691 - val_loss: 1.5866 - val_sparse_categorical_accuracy: 0.6848 - val_sparse_top_k_categorical_accuracy: 0.8694 - lr: 1.6727e-04\n",
            "Epoch 68/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4726 - sparse_categorical_accuracy: 0.8753 - sparse_top_k_categorical_accuracy: 0.9695\n",
            "Epoch 68: val_sparse_categorical_accuracy did not improve from 0.68521\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.4726 - sparse_categorical_accuracy: 0.8753 - sparse_top_k_categorical_accuracy: 0.9695 - val_loss: 1.5888 - val_sparse_categorical_accuracy: 0.6850 - val_sparse_top_k_categorical_accuracy: 0.8696 - lr: 1.5896e-04\n",
            "Epoch 69/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4658 - sparse_categorical_accuracy: 0.8772 - sparse_top_k_categorical_accuracy: 0.9701\n",
            "Epoch 69: val_sparse_categorical_accuracy improved from 0.68521 to 0.68546, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.4658 - sparse_categorical_accuracy: 0.8772 - sparse_top_k_categorical_accuracy: 0.9701 - val_loss: 1.5894 - val_sparse_categorical_accuracy: 0.6855 - val_sparse_top_k_categorical_accuracy: 0.8698 - lr: 1.5106e-04\n",
            "Epoch 70/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4605 - sparse_categorical_accuracy: 0.8779 - sparse_top_k_categorical_accuracy: 0.9707\n",
            "Epoch 70: val_sparse_categorical_accuracy did not improve from 0.68546\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.4605 - sparse_categorical_accuracy: 0.8779 - sparse_top_k_categorical_accuracy: 0.9707 - val_loss: 1.5923 - val_sparse_categorical_accuracy: 0.6849 - val_sparse_top_k_categorical_accuracy: 0.8693 - lr: 1.4356e-04\n",
            "Epoch 71/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4552 - sparse_categorical_accuracy: 0.8797 - sparse_top_k_categorical_accuracy: 0.9713\n",
            "Epoch 71: val_sparse_categorical_accuracy improved from 0.68546 to 0.68702, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 35s 36ms/step - loss: 0.4552 - sparse_categorical_accuracy: 0.8797 - sparse_top_k_categorical_accuracy: 0.9713 - val_loss: 1.5891 - val_sparse_categorical_accuracy: 0.6870 - val_sparse_top_k_categorical_accuracy: 0.8695 - lr: 1.3643e-04\n",
            "Epoch 72/100\n",
            "986/987 [============================>.] - ETA: 0s - loss: 0.4488 - sparse_categorical_accuracy: 0.8811 - sparse_top_k_categorical_accuracy: 0.9717\n",
            "Epoch 72: val_sparse_categorical_accuracy did not improve from 0.68702\n",
            "987/987 [==============================] - 39s 40ms/step - loss: 0.4488 - sparse_categorical_accuracy: 0.8811 - sparse_top_k_categorical_accuracy: 0.9717 - val_loss: 1.5906 - val_sparse_categorical_accuracy: 0.6860 - val_sparse_top_k_categorical_accuracy: 0.8700 - lr: 1.2966e-04\n",
            "Epoch 73/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4451 - sparse_categorical_accuracy: 0.8814 - sparse_top_k_categorical_accuracy: 0.9721\n",
            "Epoch 73: val_sparse_categorical_accuracy did not improve from 0.68702\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.4451 - sparse_categorical_accuracy: 0.8814 - sparse_top_k_categorical_accuracy: 0.9721 - val_loss: 1.5932 - val_sparse_categorical_accuracy: 0.6853 - val_sparse_top_k_categorical_accuracy: 0.8702 - lr: 1.2322e-04\n",
            "Epoch 74/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4397 - sparse_categorical_accuracy: 0.8830 - sparse_top_k_categorical_accuracy: 0.9727\n",
            "Epoch 74: val_sparse_categorical_accuracy did not improve from 0.68702\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.4397 - sparse_categorical_accuracy: 0.8830 - sparse_top_k_categorical_accuracy: 0.9727 - val_loss: 1.5964 - val_sparse_categorical_accuracy: 0.6863 - val_sparse_top_k_categorical_accuracy: 0.8703 - lr: 1.1711e-04\n",
            "Epoch 75/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4351 - sparse_categorical_accuracy: 0.8840 - sparse_top_k_categorical_accuracy: 0.9732\n",
            "Epoch 75: val_sparse_categorical_accuracy did not improve from 0.68702\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.4351 - sparse_categorical_accuracy: 0.8840 - sparse_top_k_categorical_accuracy: 0.9732 - val_loss: 1.5984 - val_sparse_categorical_accuracy: 0.6864 - val_sparse_top_k_categorical_accuracy: 0.8695 - lr: 1.1131e-04\n",
            "Epoch 76/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4302 - sparse_categorical_accuracy: 0.8850 - sparse_top_k_categorical_accuracy: 0.9734\n",
            "Epoch 76: val_sparse_categorical_accuracy did not improve from 0.68702\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.4302 - sparse_categorical_accuracy: 0.8850 - sparse_top_k_categorical_accuracy: 0.9734 - val_loss: 1.5942 - val_sparse_categorical_accuracy: 0.6862 - val_sparse_top_k_categorical_accuracy: 0.8698 - lr: 1.0579e-04\n",
            "Epoch 77/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4266 - sparse_categorical_accuracy: 0.8858 - sparse_top_k_categorical_accuracy: 0.9737\n",
            "Epoch 77: val_sparse_categorical_accuracy did not improve from 0.68702\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.4266 - sparse_categorical_accuracy: 0.8858 - sparse_top_k_categorical_accuracy: 0.9737 - val_loss: 1.5974 - val_sparse_categorical_accuracy: 0.6862 - val_sparse_top_k_categorical_accuracy: 0.8699 - lr: 1.0055e-04\n",
            "Epoch 78/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4229 - sparse_categorical_accuracy: 0.8864 - sparse_top_k_categorical_accuracy: 0.9741\n",
            "Epoch 78: val_sparse_categorical_accuracy did not improve from 0.68702\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.4229 - sparse_categorical_accuracy: 0.8864 - sparse_top_k_categorical_accuracy: 0.9741 - val_loss: 1.5979 - val_sparse_categorical_accuracy: 0.6869 - val_sparse_top_k_categorical_accuracy: 0.8702 - lr: 9.5575e-05\n",
            "Epoch 79/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4171 - sparse_categorical_accuracy: 0.8882 - sparse_top_k_categorical_accuracy: 0.9746\n",
            "Epoch 79: val_sparse_categorical_accuracy did not improve from 0.68702\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.4171 - sparse_categorical_accuracy: 0.8882 - sparse_top_k_categorical_accuracy: 0.9746 - val_loss: 1.5978 - val_sparse_categorical_accuracy: 0.6866 - val_sparse_top_k_categorical_accuracy: 0.8702 - lr: 9.0846e-05\n",
            "Epoch 80/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4152 - sparse_categorical_accuracy: 0.8882 - sparse_top_k_categorical_accuracy: 0.9748\n",
            "Epoch 80: val_sparse_categorical_accuracy improved from 0.68702 to 0.68723, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.4152 - sparse_categorical_accuracy: 0.8882 - sparse_top_k_categorical_accuracy: 0.9748 - val_loss: 1.5981 - val_sparse_categorical_accuracy: 0.6872 - val_sparse_top_k_categorical_accuracy: 0.8703 - lr: 8.6354e-05\n",
            "Epoch 81/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4108 - sparse_categorical_accuracy: 0.8895 - sparse_top_k_categorical_accuracy: 0.9752\n",
            "Epoch 81: val_sparse_categorical_accuracy improved from 0.68723 to 0.68736, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.4108 - sparse_categorical_accuracy: 0.8895 - sparse_top_k_categorical_accuracy: 0.9752 - val_loss: 1.5994 - val_sparse_categorical_accuracy: 0.6874 - val_sparse_top_k_categorical_accuracy: 0.8711 - lr: 8.2086e-05\n",
            "Epoch 82/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4074 - sparse_categorical_accuracy: 0.8902 - sparse_top_k_categorical_accuracy: 0.9755\n",
            "Epoch 82: val_sparse_categorical_accuracy did not improve from 0.68736\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.4074 - sparse_categorical_accuracy: 0.8902 - sparse_top_k_categorical_accuracy: 0.9755 - val_loss: 1.6032 - val_sparse_categorical_accuracy: 0.6874 - val_sparse_top_k_categorical_accuracy: 0.8707 - lr: 7.8032e-05\n",
            "Epoch 83/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4047 - sparse_categorical_accuracy: 0.8908 - sparse_top_k_categorical_accuracy: 0.9757\n",
            "Epoch 83: val_sparse_categorical_accuracy did not improve from 0.68736\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.4047 - sparse_categorical_accuracy: 0.8908 - sparse_top_k_categorical_accuracy: 0.9757 - val_loss: 1.6015 - val_sparse_categorical_accuracy: 0.6873 - val_sparse_top_k_categorical_accuracy: 0.8701 - lr: 7.4180e-05\n",
            "Epoch 84/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4013 - sparse_categorical_accuracy: 0.8915 - sparse_top_k_categorical_accuracy: 0.9761\n",
            "Epoch 84: val_sparse_categorical_accuracy did not improve from 0.68736\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.4013 - sparse_categorical_accuracy: 0.8915 - sparse_top_k_categorical_accuracy: 0.9761 - val_loss: 1.6031 - val_sparse_categorical_accuracy: 0.6872 - val_sparse_top_k_categorical_accuracy: 0.8705 - lr: 7.0521e-05\n",
            "Epoch 85/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.4000 - sparse_categorical_accuracy: 0.8916 - sparse_top_k_categorical_accuracy: 0.9761\n",
            "Epoch 85: val_sparse_categorical_accuracy did not improve from 0.68736\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.4000 - sparse_categorical_accuracy: 0.8916 - sparse_top_k_categorical_accuracy: 0.9761 - val_loss: 1.6032 - val_sparse_categorical_accuracy: 0.6873 - val_sparse_top_k_categorical_accuracy: 0.8707 - lr: 6.7045e-05\n",
            "Epoch 86/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.3960 - sparse_categorical_accuracy: 0.8928 - sparse_top_k_categorical_accuracy: 0.9766\n",
            "Epoch 86: val_sparse_categorical_accuracy improved from 0.68736 to 0.68848, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.3960 - sparse_categorical_accuracy: 0.8928 - sparse_top_k_categorical_accuracy: 0.9766 - val_loss: 1.6015 - val_sparse_categorical_accuracy: 0.6885 - val_sparse_top_k_categorical_accuracy: 0.8705 - lr: 6.3743e-05\n",
            "Epoch 87/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.3928 - sparse_categorical_accuracy: 0.8935 - sparse_top_k_categorical_accuracy: 0.9768\n",
            "Epoch 87: val_sparse_categorical_accuracy improved from 0.68848 to 0.68858, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.3928 - sparse_categorical_accuracy: 0.8935 - sparse_top_k_categorical_accuracy: 0.9768 - val_loss: 1.6047 - val_sparse_categorical_accuracy: 0.6886 - val_sparse_top_k_categorical_accuracy: 0.8705 - lr: 6.0606e-05\n",
            "Epoch 88/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.3911 - sparse_categorical_accuracy: 0.8939 - sparse_top_k_categorical_accuracy: 0.9770\n",
            "Epoch 88: val_sparse_categorical_accuracy did not improve from 0.68858\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.3911 - sparse_categorical_accuracy: 0.8939 - sparse_top_k_categorical_accuracy: 0.9770 - val_loss: 1.6064 - val_sparse_categorical_accuracy: 0.6873 - val_sparse_top_k_categorical_accuracy: 0.8707 - lr: 5.7625e-05\n",
            "Epoch 89/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.3885 - sparse_categorical_accuracy: 0.8945 - sparse_top_k_categorical_accuracy: 0.9772\n",
            "Epoch 89: val_sparse_categorical_accuracy did not improve from 0.68858\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.3885 - sparse_categorical_accuracy: 0.8945 - sparse_top_k_categorical_accuracy: 0.9772 - val_loss: 1.6064 - val_sparse_categorical_accuracy: 0.6878 - val_sparse_top_k_categorical_accuracy: 0.8710 - lr: 5.4794e-05\n",
            "Epoch 90/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.3870 - sparse_categorical_accuracy: 0.8949 - sparse_top_k_categorical_accuracy: 0.9774\n",
            "Epoch 90: val_sparse_categorical_accuracy did not improve from 0.68858\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.3870 - sparse_categorical_accuracy: 0.8949 - sparse_top_k_categorical_accuracy: 0.9774 - val_loss: 1.6058 - val_sparse_categorical_accuracy: 0.6880 - val_sparse_top_k_categorical_accuracy: 0.8707 - lr: 5.2104e-05\n",
            "Epoch 91/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.3854 - sparse_categorical_accuracy: 0.8951 - sparse_top_k_categorical_accuracy: 0.9775\n",
            "Epoch 91: val_sparse_categorical_accuracy did not improve from 0.68858\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.3854 - sparse_categorical_accuracy: 0.8951 - sparse_top_k_categorical_accuracy: 0.9775 - val_loss: 1.6063 - val_sparse_categorical_accuracy: 0.6877 - val_sparse_top_k_categorical_accuracy: 0.8711 - lr: 4.9549e-05\n",
            "Epoch 92/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.3825 - sparse_categorical_accuracy: 0.8961 - sparse_top_k_categorical_accuracy: 0.9776\n",
            "Epoch 92: val_sparse_categorical_accuracy did not improve from 0.68858\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.3825 - sparse_categorical_accuracy: 0.8961 - sparse_top_k_categorical_accuracy: 0.9776 - val_loss: 1.6069 - val_sparse_categorical_accuracy: 0.6884 - val_sparse_top_k_categorical_accuracy: 0.8709 - lr: 4.7122e-05\n",
            "Epoch 93/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.3808 - sparse_categorical_accuracy: 0.8962 - sparse_top_k_categorical_accuracy: 0.9778\n",
            "Epoch 93: val_sparse_categorical_accuracy did not improve from 0.68858\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.3808 - sparse_categorical_accuracy: 0.8962 - sparse_top_k_categorical_accuracy: 0.9778 - val_loss: 1.6083 - val_sparse_categorical_accuracy: 0.6880 - val_sparse_top_k_categorical_accuracy: 0.8716 - lr: 4.4816e-05\n",
            "Epoch 94/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.3789 - sparse_categorical_accuracy: 0.8967 - sparse_top_k_categorical_accuracy: 0.9781\n",
            "Epoch 94: val_sparse_categorical_accuracy did not improve from 0.68858\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.3789 - sparse_categorical_accuracy: 0.8967 - sparse_top_k_categorical_accuracy: 0.9781 - val_loss: 1.6084 - val_sparse_categorical_accuracy: 0.6881 - val_sparse_top_k_categorical_accuracy: 0.8709 - lr: 4.2625e-05\n",
            "Epoch 95/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.3773 - sparse_categorical_accuracy: 0.8971 - sparse_top_k_categorical_accuracy: 0.9781\n",
            "Epoch 95: val_sparse_categorical_accuracy improved from 0.68858 to 0.68861, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 35ms/step - loss: 0.3773 - sparse_categorical_accuracy: 0.8971 - sparse_top_k_categorical_accuracy: 0.9781 - val_loss: 1.6084 - val_sparse_categorical_accuracy: 0.6886 - val_sparse_top_k_categorical_accuracy: 0.8710 - lr: 4.0544e-05\n",
            "Epoch 96/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.3748 - sparse_categorical_accuracy: 0.8979 - sparse_top_k_categorical_accuracy: 0.9785\n",
            "Epoch 96: val_sparse_categorical_accuracy did not improve from 0.68861\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.3748 - sparse_categorical_accuracy: 0.8979 - sparse_top_k_categorical_accuracy: 0.9785 - val_loss: 1.6094 - val_sparse_categorical_accuracy: 0.6884 - val_sparse_top_k_categorical_accuracy: 0.8710 - lr: 3.8566e-05\n",
            "Epoch 97/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.3736 - sparse_categorical_accuracy: 0.8978 - sparse_top_k_categorical_accuracy: 0.9785\n",
            "Epoch 97: val_sparse_categorical_accuracy improved from 0.68861 to 0.68887, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.3736 - sparse_categorical_accuracy: 0.8978 - sparse_top_k_categorical_accuracy: 0.9785 - val_loss: 1.6073 - val_sparse_categorical_accuracy: 0.6889 - val_sparse_top_k_categorical_accuracy: 0.8710 - lr: 3.6688e-05\n",
            "Epoch 98/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.3726 - sparse_categorical_accuracy: 0.8983 - sparse_top_k_categorical_accuracy: 0.9786\n",
            "Epoch 98: val_sparse_categorical_accuracy improved from 0.68887 to 0.68943, saving model to ./clip-vit-large-patch14_224pix-emb64_projection.h5\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.3726 - sparse_categorical_accuracy: 0.8983 - sparse_top_k_categorical_accuracy: 0.9786 - val_loss: 1.6076 - val_sparse_categorical_accuracy: 0.6894 - val_sparse_top_k_categorical_accuracy: 0.8708 - lr: 3.4904e-05\n",
            "Epoch 99/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.3710 - sparse_categorical_accuracy: 0.8988 - sparse_top_k_categorical_accuracy: 0.9787\n",
            "Epoch 99: val_sparse_categorical_accuracy did not improve from 0.68943\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.3710 - sparse_categorical_accuracy: 0.8988 - sparse_top_k_categorical_accuracy: 0.9787 - val_loss: 1.6079 - val_sparse_categorical_accuracy: 0.6878 - val_sparse_top_k_categorical_accuracy: 0.8713 - lr: 3.3208e-05\n",
            "Epoch 100/100\n",
            "987/987 [==============================] - ETA: 0s - loss: 0.3693 - sparse_categorical_accuracy: 0.8993 - sparse_top_k_categorical_accuracy: 0.9788\n",
            "Epoch 100: val_sparse_categorical_accuracy did not improve from 0.68943\n",
            "987/987 [==============================] - 34s 34ms/step - loss: 0.3693 - sparse_categorical_accuracy: 0.8993 - sparse_top_k_categorical_accuracy: 0.9788 - val_loss: 1.6098 - val_sparse_categorical_accuracy: 0.6880 - val_sparse_top_k_categorical_accuracy: 0.8708 - lr: 3.1598e-05\n"
          ]
        }
      ],
      "source": [
        "# training projection model\n",
        "if config.TRAIN:\n",
        "    history = projection_model.fit(\n",
        "        ds_train,\n",
        "        epochs=config.EPOCHS,\n",
        "        callbacks=[get_lr_callback(), sv_loss],\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data = ds_valid,\n",
        "        validation_steps = validation_steps,\n",
        "        verbose=1\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BIS5UoYfmwg"
      },
      "source": [
        "# Step3) save model weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mmwIebmPbi5B"
      },
      "outputs": [],
      "source": [
        "# load best weight \n",
        "projection_model.load_weights( config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_projection.h5\" )\n",
        "\n",
        "# save weight of entire model\n",
        "model.save_weights(  config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_loss.h5\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WD0Qr2Q08HK"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/MyDrive/kaggle/google_embedding')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yXmhuLVfmwg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6649c6bd186a436cb29bb41fd90e9351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93ef3de69b9c466988166becd2531c27",
              "IPY_MODEL_e07f56085d40449b824a30183099caeb",
              "IPY_MODEL_b568725f98e745548498aa6e8bd2bd06"
            ],
            "layout": "IPY_MODEL_a821bfd397364f7abbae147873f11be8"
          }
        },
        "93ef3de69b9c466988166becd2531c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04a88c3c501247b1a2b8966e32414d59",
            "placeholder": "​",
            "style": "IPY_MODEL_610cd412ec434f4d85b90a6e91aab412",
            "value": "100%"
          }
        },
        "e07f56085d40449b824a30183099caeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb0d12faf09441158dca42e905f32ba0",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_67340a73358c45df8cd0b947cff96c2b",
            "value": 3
          }
        },
        "b568725f98e745548498aa6e8bd2bd06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f29d5e26e0d34a91985dc8b504941ba7",
            "placeholder": "​",
            "style": "IPY_MODEL_7073ba683b1146ea9f259cf292b19768",
            "value": " 3/3 [00:24&lt;00:00,  8.86s/it]"
          }
        },
        "a821bfd397364f7abbae147873f11be8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04a88c3c501247b1a2b8966e32414d59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "610cd412ec434f4d85b90a6e91aab412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb0d12faf09441158dca42e905f32ba0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67340a73358c45df8cd0b947cff96c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f29d5e26e0d34a91985dc8b504941ba7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7073ba683b1146ea9f259cf292b19768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2981bea196146c8b728b78902c4bfc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7472720330be426c804c42c1cd8a1782",
              "IPY_MODEL_def52a89e7e74724b1c916dfb3d06324",
              "IPY_MODEL_0a355dcc8a49424da5532ed1a7725eaf"
            ],
            "layout": "IPY_MODEL_76ca37987a8d4b5d96fc380579f3d73c"
          }
        },
        "7472720330be426c804c42c1cd8a1782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c4cb3c567f04d94822e696a7ef2ecc7",
            "placeholder": "​",
            "style": "IPY_MODEL_593f2961c59b4528b1387e44779184cd",
            "value": "100%"
          }
        },
        "def52a89e7e74724b1c916dfb3d06324": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08538074df7c4535aaa523fcb1cf8612",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee8359e474df4d8bb47799752958e999",
            "value": 3
          }
        },
        "0a355dcc8a49424da5532ed1a7725eaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6b9bbf32f2e46cb984f1aa8842f60f0",
            "placeholder": "​",
            "style": "IPY_MODEL_cd53548d4dbc41e5933498c58a9fcf0b",
            "value": " 3/3 [00:02&lt;00:00,  1.06it/s]"
          }
        },
        "76ca37987a8d4b5d96fc380579f3d73c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c4cb3c567f04d94822e696a7ef2ecc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "593f2961c59b4528b1387e44779184cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08538074df7c4535aaa523fcb1cf8612": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee8359e474df4d8bb47799752958e999": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6b9bbf32f2e46cb984f1aa8842f60f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd53548d4dbc41e5933498c58a9fcf0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}